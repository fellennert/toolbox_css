---
title: "Chapter 9: APIs"
published-title: apis
engine: knitr
freeze: auto
bibliography: literature.bib
csl: ASA.csl
---

In the prior chapter you were shown how to make calls to web pages and get responses. In this chapter, we will cover making calls to APIs which (usually) give you content in a nice and structured manner.

## Application Programming Interfaces (APIs)

```{r echo=FALSE, message=FALSE, warning=FALSE}
vembedr::embed_youtube("j64y6K2TAu4")
```

While web scraping (or *screen scraping*, as you extract the stuff that appears on your screen) is certainly fun, it should be seen as a last resort. More and more web platforms provide so-called Application Programming Interfaces (APIs).

> "An application programming interface (API) is a connection between computers or between computer programs." ([Wikipedia](https://en.wikipedia.org/wiki/API))

There are a bunch of different sorts of APIs, but the most common one is the REST API. REST stands for "REpresentational State Transfer" and describes a set of rules the API designers are supposed to obey when developing their particular interface. You can make different requests, such as *GET* content, *POST* a file to a server -- `PUT` is similar, or request to `DELETE` a file. We will only focus on the `GET` part.

APIs offer you a structured way to communicate with the platform via your machine. In our use case, this means that you can get the data you want in a usually well-structured format and without all the "dirt" that you need to scrape off tediously (enough web scraping metaphors for today). With APIs, you can generally quite clearly define what you want and how you want it. In R, we achieve this by using the `httr` [@wickham_httr_2020] package. Moreover, using APIs does not bear the risk of acquiring the information you are not supposed to access and you also do not need to worry about the server not being able to handle the load of your requests (usually, there are rate limits in place to address this particular issue). However, it's not all fun and games with APIs: they might give you their data in a special format, both XML and JSON are common. The former is the one `rvest` uses as well, the latter can be tamed using `jsonlite` [@ooms_jsonlite_2020] which is to be introduced as well. Moreover, you usually have to ask the platform for permission and perhaps pay to get it. Once you have received the keys you need, you can tell R to fill them automatically, similar to how your browser knows your Amazon password, etc.; `usethis` [@wickham_usethis_2021] can help you with such tasks.

The best thing that can happen with APIs: some of them are so popular that people have already written specific R packages for working with them -- an overview can be found on the [ROpenSci website](https://ropensci.org/packages/data-access/). One example of this was Twitter and the `rtweet` package [@kearney_rtweet_2019].

### Obtaining their data

API requests are performed using URLs. Those start with the *basic address of the API* (e.g., https://api.nytimes.com), followed by the *endpoint* that you want to use (e.g., /lists). They also contain so-called *headers* which are provided as key-value pairs. Those headers can contain for instance authentication tokens or different search parameters. A request to the New York Times API to obtain articles for January 2019 would then look like this: https://api.nytimes.com/svc/archive/v1/2019/1.json?api-key=yourkey.

At most APIs, you will have to register first. As we will play with the New York Times API, do this [here](https://developer.nytimes.com/get-started).

### Making queries

A basic query is performed using the `GET()` function. However, first, you need to define the call you want to make. The different keys and values they can take can be found in the [API documentation](https://developer.nytimes.com/docs/timeswire-product/1/overview). Of course, there is also a neater way to deal with the key problem. We will show it later.

```{r}
needs(httr, jsonlite, tidyverse)
#see overview here: https://developer.nytimes.com/docs/timeswire-product/1/overview
key <- Sys.getenv("nyt_key")

nyt_headlines <- modify_url(
  url = "https://api.nytimes.com/",
  path = "svc/news/v3/content/nyt/business.json",
  query = list(`api-key` = key)
  )

response <- GET(nyt_headlines)

response
```

When it comes to the [NYT news API](https://developer.nytimes.com/docs/timeswire-product/1/overview), there is the problem that the type of section is specified not in the query but in the endpoint path itself. Hence, if we were to scrape the different sections, we would have to change the path itself, e.g., through `str_c()` or `httr::modify_url()`.

```{r}
paths <- str_c("svc/news/v3/content/nyt/", c("business", "world"), ".json")

map(paths, 
    \(x) GET(modify_url(
      url = "https://api.nytimes.com/",
      path = x,
      query = list(`api-key` = key))
      )
    )
```

The `Status:` code you want to see here is `200` which stands for success. If you want to put it inside a function, you might want to break the function once you get a non-successful query. `http_error()` or `http_status()` are your friends here.

```{r}
response |> http_error() # can be used in if...else
response |> http_status()
```

`content()` will give you the content of the request.

```{r eval=FALSE}
response |> content() |> str()
```

What you see is also the content of the call -- which is what we want. It is in a format that we cannot work with right away, though, it is in JSON.

### JSON

The following unordered list is stolen from this [blog entry](https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/intro-to-JSON/):

-   The data are in name/value pairs
-   Commas separate data objects
-   Curly brackets {} hold objects
-   Square brackets \[\] hold arrays
-   Each data element is enclosed with quotes "" if it is a character, or without quotes if it is a numeric value

```{r}
rawToChar(response$content) |> 
  str_sub(1, 250) |> 
  writeLines() 
```

`jsonlite` helps us to bring this output into a data frame.

```{r message=FALSE, warning=FALSE}
tbl_nyt <- response |> 
  content(as = "text") |>
  jsonlite::fromJSON() 

tbl_nyt |> str(max.level = 1)
tbl_nyt |> pluck(4) |> glimpse()
```

### Dealing with authentification

Well, as we saw before, we would have to put our official NYT API key publicly visible in this script. This is bad practice and should be avoided, especially if you work on a joint project (where everybody uses their code) or if you put your scripts in public places (such as GitHub). The `usethis` package can help you here.

```{r eval=FALSE}
needs(usethis)
usethis::edit_r_environ() # save key there
Sys.getenv("nyt_key")
```

#### Exercise

1.  Search for articles on the NYT API (find the proper parameters [here](https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get)) that deal with a certain topic (parameter "q"), set a certain begin and end date. Extract the results into a tibble.

Bonus: Provide the key by using the `Sys.getenv` function. So, if somebody wants to work with your code and their own key, all they need to make sure is that they have the API key stored in the environment with the same name.

<details>
  <summary>Solution. Click to expand!</summary>
```{r}
needs(tidyverse, jsonlite, httr)
trump_nov_2016 <- modify_url(
  url = "http://api.nytimes.com/",
  path = "svc/search/v2/articlesearch.json",
  query = list(q = "Trump",
               begin_date = "20161101",
               end_date = "20161110",
               `api-key` = Sys.getenv("nyt_key"))
) |> 
  GET()

trump_nov_2016_tbl <- trump_nov_2016 |> 
  content(as = "text") |>
  fromJSON() |> 
  pluck(3, 1)

trump_nov_2016_tbl[[3]][[1]]
```
</details>

## Further links

-   [APIs for social scientists: A collaborative review](https://bookdown.org/paul/apis_for_social_scientists/)
-   A "laymen's guide" on web scraping ([blog post](https://towardsdatascience.com/functions-with-r-and-rvest-a-laymens-guide-acda42325a77))
