{
  "hash": "cded3930478f9840161d4b72688ad2e6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 8: `selenium`\"\npublished-title: selenium\nengine: knitr\nfreeze: auto\nbibliography: literature.bib\ncsl: ASA.csl\n---\n\nSometimes you will run into the problem of a website being not as \"scrape-able\" as you might have wanted it to. This might be due to Javascript, them blocking you for not being a \"real\" person, captchas, login forms, you name it. To avoid this, you can use a package called `selenium`. What it basically does is controlling a web browser. Its original purpose is to automate testing of web-based applications. However, it's also perfectly suited for helping you with your scraping endeavors.\n\n`selenium` is a Python application. An R wrapper (`RSelenium`) exists, yet its rather tedious to use, since you would likely have to run your browser out of a Docker container. And if you're on a recent Mac with a Silicon processor, these containers might not even exist yet. Therefore, in this script you will use the original Python version and run Python in this `quarto` document from your R session. For this, you can use `reticulate`, which allows you to switch between the R and the Python world. Hence, you can do your scraping in Python and your data manipulations in R and store all of this in one notebook.\n\n## Install Python using `reticulate` and `miniconda`\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"vembedr\">\n<div>\n<iframe src=\"https://www.youtube.com/embed/8DDEGESRfn8\" width=\"533\" height=\"300\" frameborder=\"0\" allowfullscreen=\"\" data-external=\"1\"></iframe>\n</div>\n</div>\n```\n\n:::\n:::\n\n\nTo get started with `selenium`, you first need to activate your Python environment. Then, you need to install the `selenium` package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreticulate::use_condaenv(\"toolbox_env\", required = TRUE)\nreticulate::conda_install(\"toolbox_env\", \"selenium\")\n```\n:::\n\n\n## `selenium`\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"vembedr\">\n<div>\n<iframe src=\"https://www.youtube.com/embed/-Hc1hejWqqQ\" width=\"533\" height=\"300\" frameborder=\"0\" allowfullscreen=\"\" data-external=\"1\"></iframe>\n</div>\n</div>\n```\n\n:::\n:::\n\n\nNow we're good to go and can start working with `selenium`. In this tutorial, you will also learn about some basics of coding in Python. Even more so than in R, we will rely on custom-made functions that we need to `def`ine as well as `for` loops. They are different in so far as we do not need to preallocate space to objects -- such as lists -- but rather can grow them iteratively (similar to `dplyr::bind_rows()`/base R's `rbind()`). What's more is that proper indentation of code is key here. In this tutorial, though, we will focus on navigating around websites and then, once we are done, grabbing their raw `html` code, which we can then read in using `xml2::read_html()` and wrangle using `rvest` functions. If you want to go full Python, you can get into `BeautifulSoup`, Python's `rvest` equivalent.\n\n### Open a browser and navigate around\n\nFirst of all, we need to open a browser. For this, we need to have one installed, I will use Firefox in this example which you can download [here](https://www.mozilla.org/en-US/firefox/new/). For a basic example, we `import` the required packages and open a website containing books that we can scrape. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n# opens a Firefox window\ndriver = webdriver.Firefox()\n\n# Navigate to the website\ndriver.get(\"https://books.toscrape.com/\")\n```\n:::\n\n\nNote that we do not have to assign anything to the `driver` object, it will adapt as we run the code. To grab the entire page, we just extract `page_source` from the driver (`driver.page_source`). Then we can write this html file into a directory by supplying the output_path. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nhtml = driver.page_source\n\noutput_path = \"temp/book_example.html\"\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(html)\n```\n:::\n\n\nYou can also take a screenshot of the page:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.save_screenshot(\"screenshot.png\")\n```\n:::\n\n\nThen we can continue with R's `rvest`, read in the document and perform our manipulations in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneeds(rvest)\nbooks <- read_html(\"temp/book_example.html\") |> \n  html_elements(\"h3 a\") |> \n  html_text2()\n\nhead(books)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A Light in the ...\"           \"Tipping the Velvet\"          \n[3] \"Soumission\"                   \"Sharp Objects\"               \n[5] \"Sapiens: A Brief History ...\" \"The Requiem Red\"             \n```\n\n\n:::\n:::\n\n\nThis is all fun and games, but we would probably want to navigate around the website and systematically scrape the pages. In this case, we would like to click the button to get to the next page. For this, we can use the `click()` function. The button to be clicked can be identified in multiple ways, here I will show you two ways to locate it: by text (i.e., which text is on the button) and by CSS selector (i.e., the thing SelectorGadget will give you).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbutton = driver.find_element(By.LINK_TEXT, \"next\")\nbutton.click()\n\nbutton = driver.find_element(By.CSS_SELECTOR, \".next a\")\nbutton.click()\n```\n:::\n\n\nEt voilà, we're on the next page. \n\nSo this now gives us the tools to automate the scraping here and we can write our first loop which navigates through the first 5 pages, grabs the html content on the go and stores it in the respective paths. To make our code clean, we define the saving function before. We also import `time` and `random` so that we can add random delays between requests:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nimport random\n\ndef save_html(html, output_path):\n  with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(html)\n            \ndef wait_random(min_secs, max_secs):\n  time.sleep(random.uniform(min_secs, max_secs))\n            \noutput_paths = []\nfor i in range(1, 6):\n    output_path = f\"temp/books_page_{i}.html\"\n    output_paths.append(output_path)\n    \nprint(output_paths)\n\ndriver.get(\"https://books.toscrape.com/\") # navigate to landing page\nfor path in output_paths:\n    html = driver.page_source\n    save_html(html, path)\n    button = driver.find_element(By.CSS_SELECTOR, \".next a\")\n    button.click()\n    wait_random(1, 3)\n```\n:::\n\n\nNow we can use R's `fs` library to check our path and read-in the html files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneeds(fs, tidyverse)\n\ndir_ls(\"temp\", regexp = \"books\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntemp/books_page_1.html temp/books_page_2.html temp/books_page_3.html \ntemp/books_page_4.html temp/books_page_5.html \n```\n\n\n:::\n\n```{.r .cell-code}\nbooks <- dir_ls(\"temp\", regexp = \"books\") |> \n  map(\\(x) read_html(x) |> \n        html_elements(\"h3 a\") |> \n        html_attr(\"title\")) |> \n  reduce(c)\n\nhead(books)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A Light in the Attic\"                 \n[2] \"Tipping the Velvet\"                   \n[3] \"Soumission\"                           \n[4] \"Sharp Objects\"                        \n[5] \"Sapiens: A Brief History of Humankind\"\n[6] \"The Requiem Red\"                      \n```\n\n\n:::\n:::\n\n\nIf we want to go back and forth -- similar to an `rvest::session()` -- we can use different functions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.back() # Go back\ntime.sleep(1) # Brief pause for demonstration\ndriver.forward() # Go forward\ntime.sleep(1)\ndriver.refresh() #refresh page\n```\n:::\n\n\nFinally, part of navigating a page is also scrolling around. This is particularly helpful if we want to (a) appear human, and (b), when we have dynamic content that unfolds as we move. Remember the IMDb example? \n\nScrolling is not straightforward, especially since it's relying on JavaScript in the background, and you will want to consider the following points: Always add waits after scrolling to allow content to load -- and check for this new content after scrolling, so that the browser stops scrolling once the limit has been reached. Also use random scolling delays to appear more human-like. We will use a [training page](https://www.scrapingcourse.com/infinite-scrolling) to illustrate how this can be achieved.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.get(\"https://www.scrapingcourse.com/infinite-scrolling\")\n```\n:::\n\n\nLet's check, without the scrolling, how many different products we see:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nelements = driver.find_elements(By.CSS_SELECTOR, \".product-name\")\nlen(elements)\n```\n:::\n\n\nThere are 12 elements. let's do one scroll to the bottom.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n   \ntime.sleep(1)\n        \nelements = driver.find_elements(By.CSS_SELECTOR, \".product-name\")\nlen(elements)\n```\n:::\n\n\nWe can do this over and over again, until there are no new elements, in a `while` loop. In the end, we store the full page's html.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.get(\"https://www.scrapingcourse.com/infinite-scrolling\")\nelements = driver.find_elements(By.CSS_SELECTOR, \".product-name\")\nstart_length = len(elements)\nnew_length = start_length + 1\n\nwhile start_length < new_length: #loop runs until there are no new elements\n  start_length = new_length\n  driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n  wait_random(1, 3)\n  new_elements = driver.find_elements(By.CSS_SELECTOR, \".product-name\")\n  new_length = len(new_elements)\n  \nsite_html = driver.page_source\n```\n:::\n\n\nSometimes you also need to scroll down to see a particular button. This can be achieved using the following command:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbutton = driver.find_element(CSS_SELECTOR, \"[CSS selector]\") # find button\n\n# Scroll to the button\ndriver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n```\n:::\n\n\nOr you need to handle some problems on a page, like in this example you need to wait for a bit for the page to refresh. It will sometimes fail on first click but work on second. For this we can define an exception. \n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.get(\"https://www.imdb.com/search/title/?title_type=feature,tv_movie,tv_special,video,tv_series,tv_miniseries&interests=in0000073\")\n\ndef click_button_with_retry(driver):\n   try:\n       button = driver.find_element(By.CLASS_NAME, \"ipc-see-more__button\")\n       button.click()\n   except:\n       time.sleep(10)\n       button = driver.find_element(By.CLASS_NAME, \"ipc-see-more__button\") \n       button.click()\n       \ni = 1\nwhile i < 4:\n  wait_random(1, 3)\n  click_button_with_retry(driver)\n  i += 1\n```\n:::\n\n\n\n### Send input to forms and boxes\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"vembedr\">\n<div>\n<iframe src=\"https://www.youtube.com/embed/yflsu3Q3stU\" width=\"533\" height=\"300\" frameborder=\"0\" allowfullscreen=\"\" data-external=\"1\"></iframe>\n</div>\n</div>\n```\n\n:::\n:::\n\n\nWith `selenium` we can also send input to web forms in an automated manner by using `send_keys()`. Let's look at IMDb for this.\n\nImagine we have a list of movies we are interested in.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmovie_list = [\"top gun\", \"pirates of the carribean\", \"fear and loathing in las vegas\", \"batman\"]\n\ndriver.get(\"https://imdb.com\")\n```\n:::\n\n\nFirst, we need to find the search box, for instance using the CSS selector. We also need to locate the button. And finally we can send our input text and click the button.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsearch_box = driver.find_element(By.CSS_SELECTOR, \"#suggestion-search\")\nsearch_button = driver.find_element(By.CSS_SELECTOR, \"#suggestion-search-button\")\n\nsearch_box.send_keys(\"top gun\")\nsearch_button.click()\n```\n:::\n\n\nIf you want to do this in a loop, make sure to clear the search box in between searches:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsearch_box.clear() \n```\n:::\n\n\n### Captcha and Error handling\n\nAnother common problem you might encounter when scraping the web is that you need to solve captchas to prove that you are human. Since `selenium` is simulating a browser, you can just intervene whenever and solve them yourselves. For this, we need to write a function that detects when human input is required, rings a little bell, and then, once you have solved the little riddle, allows you to continue. \n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef play_alert_windows():\n    import winsound\n    winsound.Beep(1000, 500)  # 1000 Hz for 500ms\n\ndef play_alert_mac():\n  import subprocess\n  subprocess.run([\"afplay\", \"/System/Library/Sounds/Ping.aiff\"])\n  \ndef play_alert_linux():\n  print(\"\\a\") # ascii bell\n        \ndef captcha_detected(driver):\n    page_source = driver.page_source.lower()\n    captcha_keywords = [\"captcha\", \"recaptcha\", \"verify you're human\", \"roboter\"]\n    return any(keyword in page_source for keyword in captcha_keywords)\n\ndef solve_captcha_manually():\n    play_alert_mac() # or play_alert_windows() if you're using a Windows machine\n    print(\"CAPTCHA detected! Please solve it manually.\")\n    input(\"Press Enter when you've solved the CAPTCHA...\")\n    print(\"Resuming scrape...\")\n    time.sleep(2)\n```\n:::\n\n\nSo let's see how this works in real life using a [demo website](https://www.google.com/recaptcha/api2/demo).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.get(\"https://www.google.com/recaptcha/api2/demo\")\n\nif captcha_detected(driver):\n        solve_captcha_manually()\nsubmit_button = driver.find_element(By.CSS_SELECTOR, \"#recaptcha-demo-submit\")\nsubmit_button.click()\n```\n:::\n\n\nThis is it for this brief introduction to `selenium`. Now that we're done, we can just close the browser using\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.quit()\n```\n:::\n\n\nOf course, there is a lot more than this tutorial does cover. Find more information -- including a 12 hour YouTube tutorial under the following links.\n\n## Further links\n\n-   [`selenium` manual](https://selenium-python.readthedocs.io)\n-   [Getting started](https://www.selenium.dev/documentation/webdriver/getting_started/first_script/)\n-   A [12 hour course on YouTube](https://www.youtube.com/watch?v=FRn5J31eAMw)\n\n## Exercises\n\nIn general, you could try all the `rvest` exercises with `selenium` to see how these things differ. Also every page is different, therefore it will probably be best if you just start with your own things. However, here is a quite tricky example.\n\n1. Driving home for Christmas. I want to visit my family over the holidays, please give me an overview of trains (\"https://bahn.de\") that go from Leipzig to Regensburg on December 20 in a `tibble` format. The tibble should contain: Date and time, number of changes, price. \n\nHint: Provide the initial search request using user input, then save the html, scroll down, and click \"spätere Verbindungen,\" save the html, etc.\n\nBonus (very tricky): do it for Dec 20 through 23 and make a visualization of the price (y-axis) over time (x-axis).\n\n<details>\n  <summary>Solution. Click to expand!</summary>\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.get(\"https://bahn.de\")\n## enter search things manually\n\noutput_paths = []\nfor i in range(1, 20):\n    output_path = f\"temp/bahn_page_{i}.html\"\n    output_paths.append(output_path)\n    \nfor path in output_paths:\n  driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n  wait_random(1, 3)\n  html = driver.page_source\n  save_html(html, path)\n  later_connections = driver.find_element(By.XPATH, '//button[normalize-space()=\"Spätere Verbindungen\"]')\n  later_connections.click()\n  wait_random(1, 3)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_names <- dir_ls(\"temp\") %>% \n  .[str_detect(., \"bahn\")] |> \n  enframe(name = NULL, value = \"file_name\") |> \n  mutate(number = str_extract(file_name, \"[0-9]{1,2}\") |> as.numeric()) |> \n  arrange(number)\n\nbahn_list <- dir_ls(\"temp\") %>% \n  .[str_detect(., \"bahn\")] |> \n  map(read_html) |> \n  set_names(dir_ls(\"temp\") %>% \n  .[str_detect(., \"bahn\")]) %>% \n  .[file_names$file_name]\n\nraw_info <- bahn_list |> \n  map(\\(x) x |> \n        html_elements(\"div.reiseplan__infos\") |>\n        html_text2())\n\nraw_price <- bahn_list |> \n  map(\\(x) x |> \n        html_elements(\"span.reise-preis__preis\") |>\n        html_text2())\n\noutput_tbl <- tibble(\n  start = raw_info |> \n    reduce(c) |> \n    str_extract(\"geplante Abfahrt[0-9]{2}\\\\:[0-9]{2}\") |> \n    str_remove(\"geplante Abfahrt\"),\n  end = raw_info |> \n    reduce(c) |> \n    str_extract(\"Ankunft[0-9]{2}\\\\:[0-9]{2}\") |> \n    str_remove(\"Ankunft\"),\n  changes = raw_info |> \n    reduce(c) |> \n    str_extract(\"Umstieg[e]?[0-9]\") |> \n    str_remove(\"Umstieg[e]?\") |> \n    as.numeric(),\n  price = raw_price |> \n    reduce(c) |> \n    str_extract(\"[0-9,]*\") |> \n    str_replace(\",\", \"\\\\.\")) |> \n  replace_na(list(changes = 0)) |> \n  #mutate(est_time_min = as.numeric((end - start))/60) |> \n  mutate(former_value = lag(start) |> \n           str_extract(\"[0-9]{2}\") |> \n           as.numeric(),\n         start_hour = str_extract(start, \"[0-9]{2}\") |> \n           as.numeric(),\n         diff_start_next = start_hour - former_value,\n         day_break = case_when(\n           diff_start_next < -18 ~ \"break\",\n           TRUE ~ NA)) |> \n  fill(day_break, .direction = \"down\") |> \n  filter(is.na(day_break)) |> \n  distinct(start, end, changes, price)\n\n## BONUS\n\ndates <- bahn_list |> \n  map(\\(x) x |> \n        html_elements(\"div.reiseloesung-heading\") |>\n        html_text2()) |> \n  map(\\(x) if(length(x) == 0) \"Fr. 20. Dez. 2024\" else x) |> \n  map(\\(x) str_c(x, collapse = \";\"))\n\noutput_tbl <- bind_cols(\n  start = raw_info |> \n    map(\\(x) x |> \n          str_extract(\"geplante Abfahrt[0-9]{2}\\\\:[0-9]{2}\") |> \n          str_remove(\"geplante Abfahrt\")) |> \n    map2(dates, \\(x, y) x |> \n           enframe(name = NULL, value = \"start\") |> \n           mutate(date = y)) |> \n    list_rbind(),\n  end = raw_info |> \n    map(\\(x) x |> \n          str_extract(\"Ankunft[0-9]{2}\\\\:[0-9]{2}\") |> \n          str_remove(\"Ankunft\") |> \n          enframe(name = NULL, value = \"end\")) |>\n    list_rbind(),\n  raw_info |> \n    map(\\(x) x |> \n          str_extract(\"Umstieg[e]?[0-9]\") |> \n          str_remove(\"Umstieg[e]?\") |> \n          enframe(name = NULL, value = \"changes\")) |> \n    list_rbind() |> \n    replace_na(list(changes = \"0\")),\n  raw_price |>\n    map(\\(x) x |> \n          str_extract(\"[0-9,]*\") |> \n          str_replace(\",\", \"\\\\.\") |> \n          enframe(name = NULL, value = \"price\")) |> \n    list_rbind()\n)\n  \n  \nlengths <- raw_info |> \n    map(\\(x) x |> \n          str_extract(\"geplante Abfahrt[0-9]{2}\\\\:[0-9]{2}\") |> \n          str_remove(\"geplante Abfahrt\")) |> \n  map(length) |> \n  reduce(c) |> \n  map2(1:19, \\(x, y) rep(y, x)) |> \n  reduce(c)\n\n\ndec_20 <- output_tbl |> \n  mutate(page = lengths) |> \n  slice(1:66) |> \n  mutate(date = \"Fr. 20. Dez. 2024\") |> \n  distinct(date, start, end, changes, price)\n\ndec_21 <- output_tbl |> \n  mutate(page = lengths) |> \n  filter(page > 5, str_detect(date, \"21\\\\.\")) |> \n  slice(21:69) |> \n  mutate(date = \"Sa. 21. Dez. 2024\") |> \n  distinct(date, start, end, changes, price)\n\ndec_22 <- output_tbl |> \n  mutate(page = lengths) |> \n  filter(str_detect(date, \"22\\\\.\")) |> \n  slice(32:78) |> \n  mutate(date = \"So. 22. Dez. 2024\") |> \n  distinct(date, start, end, changes, price)\n\ndec_23 <- output_tbl |> \n  mutate(page = lengths) |> \n  filter(str_detect(date, \"23\\\\.\")) |> \n  slice(35:100) |> \n  mutate(date = \"Mo. 23. Dez. 2024\") |> \n  distinct(date, start, end, changes, price)\n\nbind_rows(dec_20, dec_21, dec_22, dec_23) |> \n  mutate(date = str_replace(date, \" Dez\\\\. \", \"12.\") |> \n           str_remove(\"^[A-Za-z]{2}\\\\. \"),\n         date_time = str_c(date, \" \", start, \":00\") |> \n           parse_date_time(orders = \"%d.%m.%Y %H:%M:%S\"),\n         price = as.numeric(price)) |> \n  ungroup() |> \n  ggplot() +\n  geom_line(aes(date_time, price))\n```\n:::\n\n</details>\n\n\n2. Check out all the movies' pages using a loop and adequate waiting times. \n\n\n::: {.cell}\n\n```{.python .cell-code}\ndriver.get(\"https://imdb.com\")\n\nmovie_list = [\"top gun\", \"pirates of the carribean\", \"fear and loathing in las vegas\", \"batman\"] #this is how you create a list to loop over in python\n\n# for replacing spaces with underscores\ntext = \"fear and loathing\"\nnew_text = text.replace(\" \", \"_\")\nnew_text\n```\n:::\n\n\n  a. Store the results in HTML files. \n\n<details>\n  <summary>Solution. Click to expand!</summary>\n\n::: {.cell}\n\n```{.python .cell-code}\nfor movie in movie_list:\n    # Find elements inside the loop (good practice)\n    search_box = driver.find_element(By.CSS_SELECTOR, \"#suggestion-search\")\n    search_button = driver.find_element(By.CSS_SELECTOR, \"#suggestion-search-button\")\n    \n    # Clear previous search\n    search_box.clear()  # Clear previous text\n    \n    # Perform search\n    search_box.send_keys(movie)\n    wait_random(1, 3)\n    search_button.click()\n    \n    # Save results\n    movie_file = movie.replace(\" \", \"_\")\n    output_path = f\"temp/{movie_file}.html\"\n    html = driver.page_source\n    save_html(html, output_path)\n    wait_random(1, 3)\n```\n:::\n\n</details>\n\n  b. Use `rvest` to extract the exact years and titles of the results.\n  Note: if you want to store your data in a nice tibble, the vectors need to be of the same length. use this function to replace missing elements (`NULL`) in a list: `replace_null <- function(list){modify(list, \\(x) if(is.null(x)) NA else x)}`\n\n<details>\n  <summary>Solution. Click to expand!</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\nhtmls <- dir_ls(\"temp\") %>%\n  .[!str_detect(., \"books\")] |> \n  map(read_html)\n\nreplace_null <- function(list){\n  modify(list, \\(x) if(is.null(x)) NA else x)\n}\n\nextract_data <- function(html){\n  raw <- html |> \n    html_elements(\".find-title-result .ipc-metadata-list-summary-item__c\") |> \n    html_text2()\n  tibble(\n    title = map(str_split(raw, \"\\\\n\"), \\(x) pluck(x, 1)) |>\n      replace_null() |> \n      reduce(c),\n    year = map(str_split(raw, \"\\\\n\"), \\(x) pluck(x, 2)) |> \n      replace_null() |> \n      reduce(c)\n  )\n}\n\nmovie_tibble <- htmls |> \n  map(extract_data) |> \n  list_rbind()\n```\n:::\n\n</details>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/vembedr-0.1.5/css/vembedr.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}