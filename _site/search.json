[
  {
    "objectID": "lectures-week_1.html",
    "href": "lectures-week_1.html",
    "title": "Week 1: What Is Computational Social Science (CSS)?",
    "section": "",
    "text": "This lecture answers the following questions:"
  },
  {
    "objectID": "lectures-week_1.html#slides",
    "href": "lectures-week_1.html#slides",
    "title": "Week 1: What Is Computational Social Science (CSS)?",
    "section": "Slides",
    "text": "Slides\nPlease click here to download the latest version of the slides.\n\n\n\nAlternatively, read it here (probably not the best if you’re visiting this page on a mobile device):"
  },
  {
    "objectID": "1_r_index.html",
    "href": "1_r_index.html",
    "title": "Chapter 1: Preface",
    "section": "",
    "text": "Dear student,\nif you read this script, you are either participating in one of my courses on digital methods for the social sciences, or at least interested in this topic. If you have any questions or remarks regarding this script, hit me up at felix.lennert@uni-leipzig.de.\nThis script will introduce you to techniques I regard as elementary for any aspiring (computational) social scientist: the collection of digital trace data via either scraping the web or acquiring data from application programming interfaces (APIs), the analysis of text in an automated fashion (text mining), the analysis and visualization of spatial data, and the modeling of human behavior in silico (agent-based modeling).\nThe following chapters draw heavily on packages from the tidyverse (Wickham et al. 2019) and related packages. If you have not acquired sufficient familiarity yet, you can have a look at the excellent book R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023).\nI have added brief videos to each section. In these, I will briefly go through the code of the respective section and show a bit of what’s going on in there. I sometimes spontaneously elaborate a bit more on the examples at hand or show things in the data, so they may add some value. However, the script should be sufficient to provide you an understanding the concepts I introduce."
  },
  {
    "objectID": "1_r_index.html#outline",
    "href": "1_r_index.html#outline",
    "title": "Chapter 1: Preface",
    "section": "Outline",
    "text": "Outline\nThis script will unfold as follows:\nChapter 2, “Brief R Recap,” briefly introduces RStudio Projects, Quarto, tidy data and tidyr, dplyr, ggplot, functions, loops, and purrr. These techniques are vital for the things that will come next.\nChapter 3, “R vs. Python” introduces Python and the reticulate package, which allows you to use Python in an R environment. It also introduces pandas, a powerful data manipulation package for Python.\nChapter 4, “stringr and RegExes,” deals with string manipulation using, you guessed it, stringr and RegExes.\nChapters 5 and 6, “Crawling the Web and Extracting Data” and “APIs,” introduce the reader to the basics of rvest, HTML, and CSS selectors and how these can be used to acquire data from the web. Moreover, I introduce the httr package and explain how you can use it to make requests to APIs.\nChapter 7, “selenium,” introduces selenium, a Python package that allows you to control a “headless browser” – this is invaluable if you want to scrape dynamic web sites. It will also feature an introduction to reticulate, an R package that allows you to use Python code in an R environment.\nChapter 8, “Chapter 7: OCR with tesseract,” shows you how to digitize text from pdf files or images in an automated fashion using tesseract.\nChapter 9, “OpenAI whisper” focuses on the transcription of audio files. This includes diarization.\nChapter 10, “Text Preprocessing and Featurization,” touches upon the basics of bringing text into a numeric format that lends itself to quantitative analyses. It also introduces feature weighting using TF-IDF (i.e., determining which tokens matter more than others), Named-Entity Recognition, and Part-of-Speech-tagging.\nChapter 11, “Dictionary-based analysis,” covers dictionary-based text analysis.\nChapter 12, “Supervised Classification,” deals with the classification of text in a supervised manner using tidymodels.\nChapter 13, “Unsupervised Classification,” deals with the classification of text in an unsupervised manner using “classic” Laten Dirichlet Allocation, Structural Topic Models, and Seeded Topic Models.\nChapter 14, “Word Embeddings,” showcases new text analysis techniques that are based on distributional representations of words, commonly referred to as word embeddings.\nChapter 15, “BERT” demonstrates how you can use BERT for supervised classification. This script will only contain small data-sets, since these large language models require more computing power (e.g., a server with plenty of RAM as well as sufficient CPU and/or GPU power)\nChapter 16, “Local LLMs,” introduces local large language models (LLMs) and showcases how you can use them for information extraction.\nAll chapters try to deal with social scientific examples. Data sets will be provided via Dropbox, therefore the script shall run more or less out of the box. I tried to include “Further links” for the avid readers or the ones that are dissatisfied with my coverage of the content. Exercises are included, the respective solutions will be added as the course unfolds (except for the R recap, please contact me in case you are interested)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is a Quarto website for the “Forschungsseminar CSS” course at Leipzig University. It covers different techniques for the aspiring computational social scientist, hence I have dubbed it “Toolbox CSS.” You can reach me anytime at felix.lennert@uni-leipzig.de. If you’re interested in my academic work, you can visit my website.\nHere’s the official description:\nThe curriculum covers a range of topics including data management, web scraping, speech-to-text, and computational text analysis. Students will hone their R and develop skills in Python, applying these languages to real-world social science problems. The course progresses from fundamental concepts to advanced techniques, including the use of state-of-the-art AI models for text analysis.\nThe course structure consists of one lecture and one lab session per week, providing a balance of theoretical knowledge and practical application. Throughout the semester, students will benefit from hands-on coding exercises, one-on-one mentoring, and collaborative projects. The course culminates in a research paper, allowing students to apply their new skills to a topic of their choice."
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Welcome",
    "section": "Course Structure",
    "text": "Course Structure\nThe course consists of lectures introducing each week’s content and a course script that provides hands-on coding examples for the content. It is mostly containing R with some Python mixed in for good measure when no great R alternatives exist (e.g., for web scraping with Selenium, text classification with transformer models).\nAt the beginning of the course, students are encouraged to form groups based on research interests and general vibes. I require each student group to check in with me at the beginning of each week to report their progress (even if there’s nothing to report – no progress, no problem). This does not count towards any grade but rather serves the purpose of me receiving feedback on the learning experience (this is a new course!) – and will hopefully help me with providing more appropriate guidance.\nHere’s an overview of the topics covered:\n\n\n\n\n\n\n\n\n\nWEEK\nTITLE\nCONTENT\nINFORMAL TITLE\n\n\n\n\n1\nKick Off\nHousekeeping; Setting up workstation; R recap\nWhatever you want to know about CSS\n\n\n2\nBrief Intro to Python & Regexes\nPython basics (reticulate, data types, loops, functions, pandas); Regular expressions with stringr\nREGEXES – tame your data\n\n\n3\nData Acquisition I\nHow the web is written and ethics; rvest web scraping\nstealing data from websites without them noticing it\n\n\n4\nData Acquisition II\nDynamic pages and forms with selenium; APIs with httr2\nstealing MORE data from websites\n\n\n5\nData Acquisition III\nIntro to OCR and transcription\nmaking images and audio readable\n\n\n6\nData Acquisition IV\nOptical Character Recognition (tesseract); Speech transcription (OpenAI Whisper); Project discussion\nmaking the computer your transcription servant\n\n\n7\nStudent Project Week\nWork on projects in class\ntime to get your hands dirty\n\n\n8\nText as Data I\nBag of words; Sentiment analysis, TF-IDF, NER/POS\nbasic text analysis\n\n\n9\nText as Data II\nSupervised machine learning in theory and practice\nadvanced text analysis with training data\n\n\n10\nText as Data III\nUnsupervised ML (topic modeling); Remote counseling pre-Christmas\nfinding patterns without labels\n\n\n11\nText as Data IV\nMeasuring similarity and distributional hypothesis; Word embeddings\ncutting-edge text analysis with vector spaces\n\n\n12\nText as Data V\nSupervised learning on steroids (BERT); Active learning with BERT\nholy shit…transformer models\n\n\n13\nText as Data VI\nLLMs for information extraction; Local LLMs primer\nunleashing the power of large language models\n\n\n14\nPresentation Preparation Week\nWork on presentations (deadline Jan 30, 6PM)\npolish your masterpiece\n\n\n15\nPresentations & Wrap Up\nPeer-reviewed presentations; Course wrap-up\nshow off your work"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Welcome",
    "section": "Syllabus",
    "text": "Syllabus\nPlease click here to download the latest version of the syllabus.\n\n\n\nAlternatively, read it here (probably not the best if you’re visiting this page on a mobile device):"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "",
    "text": "E-mail: felix.lennert@uni-leipzig.de\nCourse hours: Tuesdays and Thursdays, 15:15 – 16:45, NSG, SR 325; for exact dates, see schedule\nReadings: see schedule\nCourse materials: see website\nStudent hours: are to be set up individually via email; my office is H3 1.07; there is a multitude of valid reasons why you should come see me – some are listed here:\n\nthings are unclear and you need help with the material\nyou want to discuss a research idea\nyou have come across a cool new paper that I might deem interesting\nyou have recommendations for me in terms of general course resources/references/structure/behavior\nyou want some career advice from someone roughly your age\nyou forgot your mensa card at home and want some coffee/tea\nin case you need some free period supplies, no email is required, you can just get them at Leonie Steinbrinker’s office (I also have a key if she’s not there), H3 1.06"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#extensions",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#extensions",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Extensions",
    "text": "Extensions\nExtensions can be granted for particular reasons. These involve, among others, internships and sickness. In the case of the former, please give me a quick heads-up so that I can arrange it (preferably with some sort of proof). If you need an extension for a different reason than the ones mentioned above, feel free to reach out anytime, and I will do my best to accommodate your needs."
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#expectations",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#expectations",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Expectations",
    "text": "Expectations\n\nThe Basics\n\nwritten in English\nfile format: PDF\nfont size 12 pt, 1.5 line spacing\nno typos, grammatical flaws, etc. (you are living in the age of helpers such as Grammarly, there are no more excuses)\nlength: between 4,000 and 8,000 words\ncite correctly and in a uniform manner. My preferred citation style is ASA. It is strongly advised to use Zotero and Quarto/Overleaf; resources can be provided upon request.\n\nStructured resembles an empirical research paper:\n\nthe introduction contains an empirical social scientific research question that is theoretically and practically motivated (i.e., showing its scientific and real-world relevance)\nthe theory section provides a brief overview of relevant prior research; clearly testable hypotheses are derived from the literature/goals for exploratory analyses are formulated\nin data and methods, the data (including acquisition strategy), as well as the analysis strategy, are described; in our case, the data consist of text, the analyses are related to the course content; data and methods need to enable valid results\nresults need to be visualized through tables and/or (gg)plots and described in the text; tables and visualizations need to be properly labeled so that they can “stand on their own”\ndiscussion of the results is performed in the light of the theoretical foundations; potential shortcomings and reach of the paper are outlined\nthe conclusion circles back to the introduction and connects it to the results; it needs to clearly answer the research question"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-1-kick-off",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-1-kick-off",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 1: Kick Off",
    "text": "Week 1: Kick Off\n\nWelcome & Housekeeping (Tue, 14 October 2025; 15:15 – 16:45; NSG, SR 325)\nNo readings.\n\n\nSetting up your workstation (Thu, 16 October 2025; 15:15 – 16:45; NSG, SR 325)\n\nAcquire access to sc.uni-leipzig.de\nR recap (the corresponding chapters can be found in the R4DS book – _online)\n\nRMarkdown/Quarto – chapters 28 & 29\ndplyr – chapter 4\ntidyr – chapter 6\nggplot2 – chapters 2 & 10 & 11 & 12\npurrr & loops in different flavors – chapter 27\nfunctional programming – chapter 26\nset up reticulate in RStudio – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-2-brief-intro-to-python-regexes",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-2-brief-intro-to-python-regexes",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 2: Brief Intro to Python & Regexes",
    "text": "Week 2: Brief Intro to Python & Regexes\n\nPython & Regexes(Tue, 21 October 2025; 15:15 – 16:45; NSG, SR 325)\n\nBrief intro to Python\n\nreticulate – how to run Python in R studio\ndata types\nloops\nfunctions\npandas\n\n\n\n\nRegular Expressions (Thu, 23 October 2025; 15:15 – 16:45; NSG, SR 325)\n\nstringr & Regular Expressions – R4DS book, online, chapters 15 & 16"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-3-data-acquisition-i",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-3-data-acquisition-i",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 3: Data Acquisition I",
    "text": "Week 3: Data Acquisition I\n\nHow the Web is Written and Ethics (Tue, 28 October 2025; 15:15 – 16:45; NSG, SR 325)\n\nStoltz and Taylor (2024) – chapter 5\nBlog post on CSS selectors – online\nBlog posts on API calls – online\n\n\n\nrvest\n\nrvest Web scraping 101 – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-4-data-acquisition-ii",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-4-data-acquisition-ii",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 4: Data Acquisition II",
    "text": "Week 4: Data Acquisition II\n\nDynamic Pages and Forms (Tue, 04 November 2025; 15:15 – 16:45; NSG, SR 325)\n\nselenium documentation – online\n\n\n\nAPIs (Thu, 13 November 2025; 15:15 – 16:45; NSG, SR 325)\n\nhttr2 documentation – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-5-data-acquisition-iii",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-5-data-acquisition-iii",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 5: Data Acquisition III",
    "text": "Week 5: Data Acquisition III\n\nIntro to OCR and Transcription (Thu, 13 November 2025; 15:15 – 16:45; NSG, SR 325)\n\nStoltz and Taylor (2024) – chapter 5"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-6-data-acquisition-iv",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-6-data-acquisition-iv",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 6: Data Acquisition IV",
    "text": "Week 6: Data Acquisition IV\n\nOptical Character Recognition and Transcription (Tue, 18 November 2025; 15:15 – 16:45; NSG, SR 325)\n\nTesseract documentation – online\nOpenAI Whisper Python package documentation – online\n\n\n\nData Acquisition Recap and Project Discussion (Thu, 20 November 2025; 15:15 – 16:45; NSG, SR 325)"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-7-student-project-week",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-7-student-project-week",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 7: Student Project Week",
    "text": "Week 7: Student Project Week\nStudents are expected to show up to class and work on their projects."
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-8-text-as-data-i",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-8-text-as-data-i",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 8: Text as Data I",
    "text": "Week 8: Text as Data I\n\nBag of Words(Tue, 02 December 2025; 15:15 – 16:45; NSG, SR 325)\n\nEvans and Aceves (2016)\nGrimmer, Roberts, and Stewart (2022), chapters 3–5, 11, & 15\nStoltz and Taylor (2024), chapters 4–9\n\n\n\nSentiment Analysis, TF-IDF, and NER/POS (Thu, 04 December 2025; 15:15 – 16:45; NSG, SR 325)\n\nGrimmer et al. (2022), chapter 11\nJurafsky and Martin (n.d.), chapter 21 – online\nSilge and Robinson (2017) – online, chapters 2 & 3"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-9-text-as-data-ii",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-9-text-as-data-ii",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 9: Text as Data II",
    "text": "Week 9: Text as Data II\n\nSupervised Machine Learning in Theory (Tue, 09 December 2025; 15:15 – 16:45; NSG, SR 325)\n\nSupervised ML\n\nBarberá et al. (2021)\nGrimmer et al. (2022), chapters 17–20\nStoltz and Taylor (2024), chapters 9 & 12\n\n\n\n\nSupervised Machine Learning in Practice (Thu, 11 December 2025; 15:15 – 16:45; NSG, SR 325)\n\nHvitfeldt and Silge (2022) – online, chapters 6 & 7\nSilge and Hvitfeldt (2019) – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-10-text-as-data-iii",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-10-text-as-data-iii",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 10: Text as Data III",
    "text": "Week 10: Text as Data III\n\nUnsupervised ML in Theory and Practice (Tue, 16 December 2025; 15:15 – 16:45; NSG, SR 325)\n\nBlei (2012)\nDiMaggio, Nag, and Blei (2013)\nGrimmer et al. (2022), chapters 10, 12–3\nStoltz and Taylor (2024), chapters 10 & 11\nSilge and Robinson (2017) – online, chapter 6\n\n\n\nRemote Counseling pre-Christmas Break (Thu, 18 December 2025; 15:15 – 16:45; NSG, SR 325)\n1-on-1 project counseling available on Zoom.\n—- CHRISTMAS BREAK —-"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-11-text-as-data-iv",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-11-text-as-data-iv",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 11: Text as Data IV",
    "text": "Week 11: Text as Data IV\n\nMeasuring Similarity and the Distributional Hypothesis (Tue, 06 January 2026; 15:15 – 16:45; NSG, SR 325)\n\nJurafsky and Martin (n.d.), chapter 6 – online\nStoltz and Taylor (2021)\n\n\n\nWord Embeddings (Thu, 08 January 2026; 15:15 – 16:45; NSG, SR 325)\n\nHvitfeldt and Silge (2022) – online, chapter 5\nStoltz and Taylor (2024), chapter 11\ntext2map: R Tools for Text Matrices – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-12-text-as-data-v",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-12-text-as-data-v",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 12: Text as Data V",
    "text": "Week 12: Text as Data V\n\nSupervised Learning on Steroids: BERT (Tue, 13 January 2026; 15:15 – 16:45; NSG, SR 325)\n\nDo, Ollion, and Shen (2022)\nLaurer et al. (2024)\nTörnberg (2023)\nWankmüller (2022)\n\n\n\nActive Learning with BERT (Thu, 15 January 2026; 15:15 – 16:45; NSG, SR 325)\n\nset up environments\nAugmented Social Scientist tutorial – online\nBERTopic – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-13-text-as-data-vi",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-13-text-as-data-vi",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 13: Text as Data VI",
    "text": "Week 13: Text as Data VI\n\nLLMs for information extraction (Tue, 20 January 2026; 15:15 – 16:45; NSG, SR 325)\n\nStuhler, Ton, and Ollion (2025)\n\n\n\nLocal LLMs – a primer (Thu, 22 January 2026; 15:15 – 16:45; NSG, SR 325)\n\nellmer documentation – online\nTutorial from IC2S2 by Etienne Ollion, Emilien Schultz, Julien Boelaert – online"
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-14-presentation-preparation-week",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-14-presentation-preparation-week",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 14: Presentation Preparation Week",
    "text": "Week 14: Presentation Preparation Week\nNo classes. Deadline for sending presentations: January 30, 6PM."
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#week-15-presentation-wrap-up-week",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#week-15-presentation-wrap-up-week",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Week 15: Presentation & Wrap Up Week",
    "text": "Week 15: Presentation & Wrap Up Week\n\nPresentations (Tue, 03 February 2026; 15:15 – 16:45; NSG, SR 325)\nNo readings.\n\n\nPresentations & Wrap-up (Thu, 05 February 2026; 15:15 – 16:45; NSG, SR 325)\nNo readings."
  },
  {
    "objectID": "syllabus/syllabus_toolboxCSS_winter2526.html#deadline-forschungsbericht",
    "href": "syllabus/syllabus_toolboxCSS_winter2526.html#deadline-forschungsbericht",
    "title": "06-002-107-3: Forschungsseminar B – Experimentelle Soziologie und Computational Social Science",
    "section": "Deadline Forschungsbericht",
    "text": "Deadline Forschungsbericht\nMarch 19, 2026 (tentative)."
  },
  {
    "objectID": "lectures-overview.html",
    "href": "lectures-overview.html",
    "title": "Overview – week by week",
    "section": "",
    "text": "This week answers the following questions:\n\nWhat does the term CSS mean?\nWhich principles underly a new CSS?\nWhich methods does CSS encompass?\nHow do we set up our computational workstation?"
  },
  {
    "objectID": "lectures-overview.html#week-1-kick-off",
    "href": "lectures-overview.html#week-1-kick-off",
    "title": "Overview – week by week",
    "section": "",
    "text": "This week answers the following questions:\n\nWhat does the term CSS mean?\nWhich principles underly a new CSS?\nWhich methods does CSS encompass?\nHow do we set up our computational workstation?"
  },
  {
    "objectID": "lectures-overview.html#week-2-brief-intro-to-python-regexes",
    "href": "lectures-overview.html#week-2-brief-intro-to-python-regexes",
    "title": "Overview – week by week",
    "section": "Week 2: Brief Intro to Python & Regexes",
    "text": "Week 2: Brief Intro to Python & Regexes\nThis week answers the following questions:\n\nHow can we run Python in RStudio using reticulate?\nWhat are the basics of Python programming?\nWhat are regular expressions and why are they powerful?\nHow can we use stringr to work with text patterns?"
  },
  {
    "objectID": "lectures-overview.html#week-3-data-acquisition-i",
    "href": "lectures-overview.html#week-3-data-acquisition-i",
    "title": "Overview – week by week",
    "section": "Week 3: Data Acquisition I",
    "text": "Week 3: Data Acquisition I\nThis week answers the following questions:\n\nHow is the web written and structured?\nWhat are considerations in terms of law and ethics when scraping?\nHow do we acquire digital trace data via web scraping with rvest?\nWhat are CSS selectors and how do they work?"
  },
  {
    "objectID": "lectures-overview.html#week-4-data-acquisition-ii",
    "href": "lectures-overview.html#week-4-data-acquisition-ii",
    "title": "Overview – week by week",
    "section": "Week 4: Data Acquisition II",
    "text": "Week 4: Data Acquisition II\nThis week answers the following questions:\n\nHow do we scrape dynamic web pages that use JavaScript?\nHow can we interact with forms and buttons using selenium?\nWhat are APIs and how do we use them?\nHow do we make API calls with httr2?"
  },
  {
    "objectID": "lectures-overview.html#week-5-data-acquisition-iii",
    "href": "lectures-overview.html#week-5-data-acquisition-iii",
    "title": "Overview – week by week",
    "section": "Week 5: Data Acquisition III",
    "text": "Week 5: Data Acquisition III\nThis week answers the following questions:\n\nWhat is Optical Character Recognition (OCR)?\nHow can we extract text from images and PDFs?\nWhat is speech-to-text transcription?\nHow can we leverage these techniques for social science research?"
  },
  {
    "objectID": "lectures-overview.html#week-6-data-acquisition-iv",
    "href": "lectures-overview.html#week-6-data-acquisition-iv",
    "title": "Overview – week by week",
    "section": "Week 6: Data Acquisition IV",
    "text": "Week 6: Data Acquisition IV\nThis week answers the following questions:\n\nHow do we digitize text using Tesseract?\nHow do we digitize speech using OpenAI Whisper?\nWhat are best practices for data acquisition projects?\nHow do we plan our research projects?"
  },
  {
    "objectID": "lectures-overview.html#week-7-student-project-week",
    "href": "lectures-overview.html#week-7-student-project-week",
    "title": "Overview – week by week",
    "section": "Week 7: Student Project Week",
    "text": "Week 7: Student Project Week\nThis week is dedicated to working on your research projects in class.\n\nDiscuss your project ideas with peers and the instructor\nBegin data acquisition for your research\nTroubleshoot technical challenges\nForm study groups with matching research interests"
  },
  {
    "objectID": "lectures-overview.html#week-8-text-as-data-i",
    "href": "lectures-overview.html#week-8-text-as-data-i",
    "title": "Overview – week by week",
    "section": "Week 8: Text as Data I",
    "text": "Week 8: Text as Data I\nThis week answers the following questions:\n\nWhat do we mean by “text as data”?\nWhat do we mean by “bag of words”?\nHow do we perform sentiment analysis?\nWhat are TF-IDF, Named Entity Recognition (NER), and Part-of-Speech (POS) tagging?"
  },
  {
    "objectID": "lectures-overview.html#week-9-text-as-data-ii",
    "href": "lectures-overview.html#week-9-text-as-data-ii",
    "title": "Overview – week by week",
    "section": "Week 9: Text as Data II",
    "text": "Week 9: Text as Data II\nThis week answers the following questions:\n\nWhat is supervised machine learning?\nHow can we train classifiers to categorize text?\nWhat are best practices for supervised ML in social science?\nHow do we evaluate model performance?"
  },
  {
    "objectID": "lectures-overview.html#week-10-text-as-data-iii",
    "href": "lectures-overview.html#week-10-text-as-data-iii",
    "title": "Overview – week by week",
    "section": "Week 10: Text as Data III",
    "text": "Week 10: Text as Data III\nThis week answers the following questions:\n\nWhat is unsupervised machine learning?\nHow does topic modeling work?\nWhat can probabilistic topic models tell us about text corpora?\nWhen should we use unsupervised vs. supervised approaches?"
  },
  {
    "objectID": "lectures-overview.html#week-11-text-as-data-iv",
    "href": "lectures-overview.html#week-11-text-as-data-iv",
    "title": "Overview – week by week",
    "section": "Week 11: Text as Data IV",
    "text": "Week 11: Text as Data IV\nThis week answers the following questions:\n\nHow can we go beyond the “bag of words”?\nWhat is the distributional hypothesis?\nHow can we measure semantic similarity between texts?\nWhat are word embeddings and what do they capture?"
  },
  {
    "objectID": "lectures-overview.html#week-12-text-as-data-v",
    "href": "lectures-overview.html#week-12-text-as-data-v",
    "title": "Overview – week by week",
    "section": "Week 12: Text as Data V",
    "text": "Week 12: Text as Data V\nThis week answers the following questions:\n\nWhat are transformer models like BERT?\nHow do these models revolutionize text classification?\nWhat is transfer learning and why is it so powerful?\nWhat is active learning and how can it reduce annotation burden?"
  },
  {
    "objectID": "lectures-overview.html#week-13-text-as-data-vi",
    "href": "lectures-overview.html#week-13-text-as-data-vi",
    "title": "Overview – week by week",
    "section": "Week 13: Text as Data VI",
    "text": "Week 13: Text as Data VI\nThis week answers the following questions:\n\nWhat’s the latest in Natural Language Processing with Large Language Models (LLMs)?\nHow can we use LLMs for information extraction?\nWhat are local LLMs and when should we use them?\nHow do we move from codebooks to promptbooks?"
  },
  {
    "objectID": "lectures-overview.html#week-14-presentation-preparation-week",
    "href": "lectures-overview.html#week-14-presentation-preparation-week",
    "title": "Overview – week by week",
    "section": "Week 14: Presentation Preparation Week",
    "text": "Week 14: Presentation Preparation Week\nThis week is dedicated to preparing your final presentations.\n\nFinalize your analyses and preliminary results\nPrepare presentation slides (deadline: January 30, 6PM)\nReview the peer review guidelines\nPractice your 10-minute presentation"
  },
  {
    "objectID": "lectures-overview.html#week-15-presentations-wrap-up",
    "href": "lectures-overview.html#week-15-presentations-wrap-up",
    "title": "Overview – week by week",
    "section": "Week 15: Presentations & Wrap Up",
    "text": "Week 15: Presentations & Wrap Up\nThis week features peer-reviewed presentations of your research projects.\n\nPresent your research (10 minutes)\nReceive feedback from assigned peer reviewers (5 minutes)\nDiscuss next steps for your final paper\nReflect on what we’ve learned throughout the semester"
  },
  {
    "objectID": "3_reticulate.html",
    "href": "3_reticulate.html",
    "title": "Chapter 3: reticulate",
    "section": "",
    "text": "The reticulate package provides a comprehensive set of tools for interoperability between Python and R. It allows you to call Python from R in a variety of ways, including importing Python modules, sourcing Python scripts, and using Python interactively from R.\n\nneeds(reticulate, tidyverse)\n\n\n\nBefore you can use Python in R, you need to ensure Python is installed on your system. The reticulate package will try to find Python automatically, but you can also specify which Python to use.\nFor a consistent experience across Windows and Mac, I recommend using reticulate’s built-in miniconda installation helper:\n\ninstall_miniconda()\n\nThis will install a minimal version of conda, which is a package and environment management system that works well for both R and Python.\nI would recommend using a conda environment for managing Python packages, as it helps avoid conflicts. When creating the environment, you can specify the Python version (this is important since not all Python versions are compatible with all package versions, unlike R) and packages you need. For example, to create an environment named “toolbox_env” with common data science packages:\n\nconda_create(\"toolbox_env\", packages = c(\"python=3.9\", \"numpy\"))\n\nIf you work in Positron, you might need to go to settings and select the Python interpreter from this environment. In RStudio, this can be done by navigating to Tools -&gt; Global Options -&gt; Python and selecting the appropriate interpreter. An overview of all conda environments and their paths can be found with:\n\nconda_list()\n\n          name\n1         base\n2 selenium_env\n3  toolbox_env\n                                                                      python\n1                   /Users/felixlennert/Library/r-miniconda-arm64/bin/python\n2 /Users/felixlennert/Library/r-miniconda-arm64/envs/selenium_env/bin/python\n3  /Users/felixlennert/Library/r-miniconda-arm64/envs/toolbox_env/bin/python\n\n\nSide note: you can remove an environment with:\n\nconda_remove(\"r-reticulate\")\n\n\n\n\nBefore you can get started, you need to tell reticulate to use this environment:\n\nuse_condaenv(\"toolbox_env\", required = TRUE)\n\nFurther, post-creation, you can install Python packages into this environment directly from R:\n\npy_install(c(\"pandas\", \"scikit-learn\"))\n\nIn Quarto documents, like this one, you can use Python chunks directly by using the {python} engine (i.e., by typing ```{python} instead of ```{r} when creating a code block):\n\n# This is a Python chunk\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'score': [85.5, 92.0, 78.5, 88.0]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n      name  age  score\n0    Alice   25   85.5\n1      Bob   30   92.0\n2  Charlie   35   78.5\n3    David   40   88.0\n\n\nThe objects created in Python chunks are accessible in R if you make them globally accessible. For example, the df DataFrame created above can be accessed in R as follows:\n\ndf_r &lt;- py$df\nprint(df_r)\n\n     name age score\n1   Alice  25  85.5\n2     Bob  30  92.0\n3 Charlie  35  78.5\n4   David  40  88.0\n\n\nHowever, note that this is only the case when knitting the document. If you run the chunks interactively, the objects won’t be available in R.\nYou can enable availability behavior while coding by running Python code directly from R using the py_run_string() function:\n\npy_run_string(\"\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'score': [85.5, 92.0, 78.5, 88.0]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\")\n\n      name  age  score\n0    Alice   25   85.5\n1      Bob   30   92.0\n2  Charlie   35   78.5\n3    David   40   88.0\n\n# This should work\nprint(py$df)\n\n     name age score\n1   Alice  25  85.5\n2     Bob  30  92.0\n3 Charlie  35  78.5\n4   David  40  88.0\n\n\nThat’s a bit cumbersome for larger code blocks, so you can also use py_run_file() to source a Python script. Note that the output objects might differ in type when accessed from R. For example, this pandas data frame becomes a named list in R.\n\npy_run_file(\"data/create_name_tbl.py\")\n\ndata_list &lt;- py$data\ntypeof(data_list)\n\n[1] \"list\"\n\n\nThis is however easily fixable:\n\ndata_list |&gt; bind_cols()\n\n# A tibble: 4 × 3\n  name      age score\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 Alice      25  85.5\n2 Bob        30  92  \n3 Charlie    35  78.5\n4 David      40  88  \n\n\nAnother way of sharing objects between R and Python is of course by writing a file to disc. You can save data frames as CSV files in R and read them into Python, or vice versa. Make sure to set index=False when saving from Python to avoid an extra index column.\n\ndf.to_csv(\"data/people.csv\", index=False)\ndf.to_csv(\"data/people_w_index.csv\", index=True)\n\n\nread_csv(\"data/people.csv\")\n\nRows: 4 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (2): age, score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 3\n  name      age score\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Alice      25  85.5\n2 Bob        30  92  \n3 Charlie    35  78.5\n4 David      40  88  \n\nread_csv(\"data/people_w_index.csv\")\n\nNew names:\nRows: 4 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): name dbl (3): ...1, age, score\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n# A tibble: 4 × 4\n   ...1 name      age score\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     0 Alice      25  85.5\n2     1 Bob        30  92  \n3     2 Charlie    35  78.5\n4     3 David      40  88  \n\n\n\n\n\n\nreticulate documentation"
  },
  {
    "objectID": "3_reticulate.html#using-python-in-r-with-reticulate",
    "href": "3_reticulate.html#using-python-in-r-with-reticulate",
    "title": "Chapter 3: reticulate",
    "section": "",
    "text": "The reticulate package provides a comprehensive set of tools for interoperability between Python and R. It allows you to call Python from R in a variety of ways, including importing Python modules, sourcing Python scripts, and using Python interactively from R.\n\nneeds(reticulate, tidyverse)\n\n\n\nBefore you can use Python in R, you need to ensure Python is installed on your system. The reticulate package will try to find Python automatically, but you can also specify which Python to use.\nFor a consistent experience across Windows and Mac, I recommend using reticulate’s built-in miniconda installation helper:\n\ninstall_miniconda()\n\nThis will install a minimal version of conda, which is a package and environment management system that works well for both R and Python.\nI would recommend using a conda environment for managing Python packages, as it helps avoid conflicts. When creating the environment, you can specify the Python version (this is important since not all Python versions are compatible with all package versions, unlike R) and packages you need. For example, to create an environment named “toolbox_env” with common data science packages:\n\nconda_create(\"toolbox_env\", packages = c(\"python=3.9\", \"numpy\"))\n\nIf you work in Positron, you might need to go to settings and select the Python interpreter from this environment. In RStudio, this can be done by navigating to Tools -&gt; Global Options -&gt; Python and selecting the appropriate interpreter. An overview of all conda environments and their paths can be found with:\n\nconda_list()\n\n          name\n1         base\n2 selenium_env\n3  toolbox_env\n                                                                      python\n1                   /Users/felixlennert/Library/r-miniconda-arm64/bin/python\n2 /Users/felixlennert/Library/r-miniconda-arm64/envs/selenium_env/bin/python\n3  /Users/felixlennert/Library/r-miniconda-arm64/envs/toolbox_env/bin/python\n\n\nSide note: you can remove an environment with:\n\nconda_remove(\"r-reticulate\")\n\n\n\n\nBefore you can get started, you need to tell reticulate to use this environment:\n\nuse_condaenv(\"toolbox_env\", required = TRUE)\n\nFurther, post-creation, you can install Python packages into this environment directly from R:\n\npy_install(c(\"pandas\", \"scikit-learn\"))\n\nIn Quarto documents, like this one, you can use Python chunks directly by using the {python} engine (i.e., by typing ```{python} instead of ```{r} when creating a code block):\n\n# This is a Python chunk\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'score': [85.5, 92.0, 78.5, 88.0]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n      name  age  score\n0    Alice   25   85.5\n1      Bob   30   92.0\n2  Charlie   35   78.5\n3    David   40   88.0\n\n\nThe objects created in Python chunks are accessible in R if you make them globally accessible. For example, the df DataFrame created above can be accessed in R as follows:\n\ndf_r &lt;- py$df\nprint(df_r)\n\n     name age score\n1   Alice  25  85.5\n2     Bob  30  92.0\n3 Charlie  35  78.5\n4   David  40  88.0\n\n\nHowever, note that this is only the case when knitting the document. If you run the chunks interactively, the objects won’t be available in R.\nYou can enable availability behavior while coding by running Python code directly from R using the py_run_string() function:\n\npy_run_string(\"\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'age': [25, 30, 35, 40],\n    'score': [85.5, 92.0, 78.5, 88.0]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\")\n\n      name  age  score\n0    Alice   25   85.5\n1      Bob   30   92.0\n2  Charlie   35   78.5\n3    David   40   88.0\n\n# This should work\nprint(py$df)\n\n     name age score\n1   Alice  25  85.5\n2     Bob  30  92.0\n3 Charlie  35  78.5\n4   David  40  88.0\n\n\nThat’s a bit cumbersome for larger code blocks, so you can also use py_run_file() to source a Python script. Note that the output objects might differ in type when accessed from R. For example, this pandas data frame becomes a named list in R.\n\npy_run_file(\"data/create_name_tbl.py\")\n\ndata_list &lt;- py$data\ntypeof(data_list)\n\n[1] \"list\"\n\n\nThis is however easily fixable:\n\ndata_list |&gt; bind_cols()\n\n# A tibble: 4 × 3\n  name      age score\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 Alice      25  85.5\n2 Bob        30  92  \n3 Charlie    35  78.5\n4 David      40  88  \n\n\nAnother way of sharing objects between R and Python is of course by writing a file to disc. You can save data frames as CSV files in R and read them into Python, or vice versa. Make sure to set index=False when saving from Python to avoid an extra index column.\n\ndf.to_csv(\"data/people.csv\", index=False)\ndf.to_csv(\"data/people_w_index.csv\", index=True)\n\n\nread_csv(\"data/people.csv\")\n\nRows: 4 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (2): age, score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 3\n  name      age score\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Alice      25  85.5\n2 Bob        30  92  \n3 Charlie    35  78.5\n4 David      40  88  \n\nread_csv(\"data/people_w_index.csv\")\n\nNew names:\nRows: 4 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): name dbl (3): ...1, age, score\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n# A tibble: 4 × 4\n   ...1 name      age score\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     0 Alice      25  85.5\n2     1 Bob        30  92  \n3     2 Charlie    35  78.5\n4     3 David      40  88  \n\n\n\n\n\n\nreticulate documentation"
  },
  {
    "objectID": "2_r_catch-up.html",
    "href": "2_r_catch-up.html",
    "title": "Chapter 2: Brief R Recap",
    "section": "",
    "text": "I assume your familiarity with R. However, I am fully aware that nobody can have all these things avaible in their head all the time (that’s what they invented StackOverflow for, new AI helpers are also pretty good). In the following, I show some basics of how I use R (i.e., RStudio Projects, scripts, Quarto) as well as some data wrangling stuff (readr, tidyr, dplyr), visualization with ggplot2, functions, loops, and purrr. If you need more info, check out the “further links” I have added after each section. There are also exercises after each section.\nneeds(tidyverse, lubridate, fs, socviz)"
  },
  {
    "objectID": "2_r_catch-up.html#rstudio-projects",
    "href": "2_r_catch-up.html#rstudio-projects",
    "title": "Chapter 2: Brief R Recap",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nMotivation\nDisclaimer: those things might not be entirely clear right away. However, I am deeply convinced that it is important that you use R and RStudio properly from the start. Otherwise it won’t be as easy to re-build the right habits.\nIf you analyze data with R, one of the first things you do is to load in the data that you want to perform your analyses on. Then, you perform your analyses on them, and save the results in the (probably) same directory.\nWhen you load a data set into R, you might use the readr package and do read_csv(absolute_file_path.csv). This becomes fairly painful if you need to read in more than one data set. Then, relative paths (i.e., where you start from a certain point in your file structure, e.g., your file folder) become more useful. How you CAN go across this is to use the setwd(absolute_file_path_to_your_directory) function. Here, set stands for set and wd stands for working directory. If you are not sure about what the current working directory actually is, you can use getwd() which is the equivalent to setwd(file_path). This enables you to read in a data set – if the file is in the working directory – by only using read_csv(file_name.csv).\nHowever, if you have ever worked on an R project with other people in a group and exchanged scripts regularly, you may have encountered one of the big problems with this setwd(file_path) approach: as it only takes absolute paths like this one: “/Users/felixlennert/Library/Mobile Documents/comappleCloudDocs/phd/teaching/hhs-stockholm/fall2021/scripts/”, no other person will be able to run this script without making any changes1. Just to be clear: there are no two machines which have the exact same file structure.\nThis is where RStudio Projects come into play: they make every file path relative. The Project file (ends with .Rproj) basically sets the working directory to the folder it is in. Hence, if you want to send your work to a peer or a teacher, just send a folder which also contains the .Rproj file and they will be able to work on your project without the hassle of pasting file paths into setwd() commands.\n\n\nHow to create an RStudio Project?\nI strongly suggest that you set up a project which is dedicated to this course.\n\nIn RStudio, click File &gt;&gt; New Project…\nA window pops up which lets you select between “New Directory”, “Existing Directory”, and “Version Control.” The first option creates a new folder which is named after your project, the second one “associates a project with an existing working directory,” and the third one only applies to version control (like, for instance, GitHub) users. I suggest that you click “New Directory”.\nNow you need to specify the type of the project (Empty project, R package, or Shiny Web Application). In our case, you will need a “new project.” Hit it!\n\nThe final step is to choose the folder the project will live in. If you have already created a folder which is dedicated to this course, choose this one, and let the project live in there as a sub-directory.\nWhen you write code for our course in the future, you first open the R project – by double-clicking the .Rproj file – and then create either a new script or open a former one (e.g., by going through the “Files” tab in the respective pane which will show the right directory already.)"
  },
  {
    "objectID": "2_r_catch-up.html#r-scripts-and-quarto",
    "href": "2_r_catch-up.html#r-scripts-and-quarto",
    "title": "Chapter 2: Brief R Recap",
    "section": "R scripts and Quarto",
    "text": "R scripts and Quarto\nIn this course, you will work with two sorts of documents to store your code in: R scripts (suffix .R) and Quarto documents (suffix .qmd). In the following, I will briefly introduce you to both of them.\n\nR scripts\nThe console, where you can only execute your code, is great for experimenting with R. If you want to store it – e.g., for sharing – you need something different. This is where R scripts come in handy. When you are in RStudio, you create a new script by either clicking File &gt;&gt; New File &gt;&gt; R Script or ctrl/cmd+shift+n. There are multiple ways to run code in the script:\n\ncmd/ctrl+return (Mac/Windows) – execute entire expression and jump to next line\n\noption/alt+return (Mac/Windows) – execute entire expression and remain in line\n\ncmd/ctrl+shift+return (Mac/Windows) – execute entire script from the beginning to the end (rule: every script you hand in or send to somebody else should run smoothly from the beginning to the end)\n\nIf you want to make annotations to your code (which you should do because it makes everything easier to read and understand), just insert ‘#’ into your code. Every expression that stands to the right of the ‘#’ sign will not be executed when you run the code.\n\n\nQuarto\nA time will come where you will not just do analyses for yourself in R, but you will also have to communicate them. Let’s take a master’s thesis as an example: you need a type of document that is able to encapsulate: text (properly formatted), visualizations (tables, graphs, maybe images), and references. An RMarkdown document can do it all, plus, your entire analysis can live in there as well. So there is no need anymore for the cumbersome process of copying data from MS Excel or IBM SPSS into an MS Word table. You just tell RMarkdown what it should communicate and what not.\nIn the following, I will not provide you with an exhaustive introduction to RMarkdown. Instead, I will focus on getting you started and then referring you to better, more exhaustive resources. It is not that I am too lazy to write a big tutorial, but there are state-of-the-art tutorials and resources (which mainly come straight from people who work on the forefront of the development of these tools) which are available for free. By linking to them, I want to encourage you to get involved and dig into this stuff. So, let’s get you started!\nYou create a Quarto document file by clicking File &gt;&gt; New File &gt;&gt; Quarto Document…. Then, a window pops up that looks like this:\n\n\n\nNew Quarto\n\n\nNote that you could also do a presentation (with the beamer package), a shiny app, a website (like this one), or use templates. We will focus on simple Quarto documents. Here, you can type in a title, the name(s) of the author(s), and choose the default output format. For now you have to choose one, but later you can switch to one of the others whenever you want to.\n\nHTML is handy for lightweight, quickly rendered files, or if you want to publish it on a website.\nPDF is good if you are experienced with LaTeX and want to further modify it in terms of formatting etc., or simply want to get a more formally looking document (I use it if I need to hand in something that is supposed to be graded). If you want to knit to PDF, you need a running LaTeX version on your machine. If you do not have one, I recommend you to install tinytex. I linked installation instructions down below.\nWord puts out an MS Word document – especially handy if you collaborate with people who are either not experienced in R, like older faculty, or want some parts to be proof-read (remember the Track-Changes function?). Note that you need to have MS Word or LibreOffice installed on your machine.\n\nDid you notice the term render? The logic behind Quarto documents is that you edit them in RStudio and then render them. This means that it calls the knitr package. Thereby, all the code you include into the document is executed from scratch. If the code does not work and throws an error, the document will not knit – hence, it needs to be properly written to avoid head-scratching. The knitr package creates a markdown file (suffix: .md). This is then processed by pandoc, a universal document converter. The big advantage of this two-step approach is that it enables a wide range of output formats.\nFor your first Quarto document, choose HTML and click “OK”. Then, you see a new plain-text file which looks like this:\n\n\n\nA fresh and clean Quarto document\n\n\nThe visual editor is quite similar to what we know from word processing software such as Microsoft Word. I will run you through the features in a quick video.\n\n\nFurther links\n\nChapter on Scripts and Projects in R4DS\nMore on RStudio Projects on the posit website\nChapter on Quarto in R4DS\nAll things Quarto on its dedicated website\nYihui Xie published a manual for installing the tinytex package\n\n\n\nExercises\n\nCreate a project for this course.\nCreate a Quarto file to work on the exercises. Add the exercises and answer them in code in the document.\nRender it. Does it work?"
  },
  {
    "objectID": "2_r_catch-up.html#reading-data-into-r",
    "href": "2_r_catch-up.html#reading-data-into-r",
    "title": "Chapter 2: Brief R Recap",
    "section": "Reading data into R",
    "text": "Reading data into R\nData is typically stored in csv-files and can be read in using readr. For “normal,” comma-separated values read_csv(\"file_path\") suffices. Sometimes, a semicolon is used instead of a comma (e.g., in countries that use the commas as a decimal sign). For these files, read_csv2(\"file_path) is the way to go.\n\ntwitter_edgelist &lt;- read_csv(\"data/edgelist_sen_twitter.csv\")#,\n#                            col_types = cols(from = col_character(),\n#                                             to = col_character()))\n\nIf you encounter other data types, you just need to find the right tidyverse package to read the data in. Their syntax will be the same, it will just be the function names that differ.\n\nFurther links\n\nthe section on reading in data in R4DS\n\n\n\nExercises\nFirst, download and extract the zip file by clicking the link. Then…\nRead them in using the right functions. Specify the parameters properly. Hints can be found in hints.md. Each file should be stored in an object, names should correspond to the file names.\nNote: this is challenging, absolutely. If you have problems, try to google the different functions and think about what the different parameters indicate. If that is to no avail, send me an e-mail. I am very happy to provide you further assistance."
  },
  {
    "objectID": "2_r_catch-up.html#tidy-data-with-tidyr",
    "href": "2_r_catch-up.html#tidy-data-with-tidyr",
    "title": "Chapter 2: Brief R Recap",
    "section": "Tidy data with tidyr",
    "text": "Tidy data with tidyr\nBefore you learn how to tidy and wrangle data, you need to know how you want your data set to actually look like, i.e., what the desired outcome of the entire process of tidying your data set is. The tidyverse is a collection of packages which share an underlying philosophy: they are tidy. This means, that they (preferably) take tidy data as inputs and output tidy data. In the following, I will, first, introduce you to the concept of tidy data as developed by Hadley Wickham (Wickham 2014). Second, tidyr is introduced (Wickham 2020b). Its goal is to provide you with functions that facilitate tidying data sets. Beyond, I will provide you some examples of how to create tibbles using functions from the tibble package (Müller, Wickham, and François 2020). Moreover, the pipe is introduced.\nPlease note that tidying and cleaning data are not equivalent: I refer to tidying data as to bringing data in a tidy format. Cleaning data, however, can encompass way more than this: parsing columns in the right format (using readr, for instance), imputation of missing values, address the problem of typos, etc.\n\nThe concept of tidy data\ndata sets can be structured in many ways. To make them tidy, they must be organized in the following way (this is taken from the R for Data Science book (wickham_r_2016?)):\n\nEach variable must have its own column.\n\nEach observation must have its own row.\n\nEach value must have its own cell.\n\nThey can even be boiled further down:\n\nPut each data set in a tibble.\nPut each variable in a column.\n\nThis can also be visually depicted:\n\n\n\nThe three rules that make a data set tidy (taken from Wickham and Grolemund 2016: 149)\n\n\nThis way of storing data has two big advantages:\n\nyou can easily access, and hence manipulate, variables as vectors\nif you perform vectorized operations on the tibble, cases are preserved.\n\n\n\nMaking messy data tidy\nSo what are the most common problems with data sets? The following list is taken from the tidyr vignette2:\n\nColumn headers are values, not variable names.\n\nVariables are stored in both rows and columns.\n\nMultiple variables are stored in one column.\n\nMultiple types of observational units are stored in the same table.\n\nA single observational unit is stored in multiple tables.\n\nI will go across the former three types of problems, because the latter two require some more advanced data wrangling techniques you haven’t learned yet (i.e., functions from the dplyr package: select(), mutate(), left_join(), among others).\nIn the following, I will provide you with examples on how this might look like and how you can address the respective problem using functions from the tidyr package. This will serve as an introduction to the two most important functions of the tidyr package: pivot_longer() and its counterpart pivot_wider(). Beyond that, separate() will be introduced as well. At the beginning of every part, I will build the tibble using functions from the tibble package. This should suffice as a quick refresher for and introduction to creating tibbles.\ntidyr has some more functions in stock. They do not necessarily relate to transforming messy data sets into tidy ones, but also serve you well for some general cleaning tasks. They will be introduced, too.\n\nColumn headers are values\nA data set of this form would look like this:\n\ntibble_value_headers &lt;- tibble(\n  manufacturer = c(\"Audi\", \"BMW\", \"Mercedes\", \"Opel\", \"VW\"),\n  `3 cyl` = sample(20, 5, replace = TRUE),\n  `4 cyl` = sample(50:100, 5, replace = TRUE),\n  `5 cyl` = sample(10, 5, replace = TRUE),\n  `6 cyl` = sample(30:50, 5, replace = TRUE),\n  `8 cyl` = sample(20:40, 5, replace = TRUE),\n  `10 cyl` = sample(10, 5, replace = TRUE),\n  `12 cyl` = sample(20, 5, replace = TRUE),\n  `16 cyl` = rep(0, 5)\n)\n\ntibble_value_headers\n\n# A tibble: 5 × 9\n  manufacturer `3 cyl` `4 cyl` `5 cyl` `6 cyl` `8 cyl` `10 cyl` `12 cyl`\n  &lt;chr&gt;          &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n1 Audi               6      76       9      47      39        3       18\n2 BMW               11      78       7      41      40       10        2\n3 Mercedes           3      84      10      32      36        7        4\n4 Opel              16      56       3      37      40        4       13\n5 VW                 7      70       1      45      24        3       16\n# ℹ 1 more variable: `16 cyl` &lt;dbl&gt;\n\n\nYou can create a tibble by column using the tibble function. Column names need to be specified and linked to vectors of either the same length or length one.\nThis data set basically consists of three variables: German car manufacturer, number of cylinders, and frequency. To make the data set tidy, it has to consist of three columns depicting the three respective variables. This operation is called pivoting the non-variable columns into two-column key-value pairs. As the data set will thereafter contain fewer columns and more rows than before, it will have become longer (or taller). Hence, the tidyr function is called pivot_longer().\n\nger_car_manufacturer_longer &lt;- tibble_value_headers |&gt; \n  pivot_longer(-manufacturer, names_to = \"cylinders\", values_to = \"frequency\")\nger_car_manufacturer_longer\n\n# A tibble: 40 × 3\n   manufacturer cylinders frequency\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt;\n 1 Audi         3 cyl             6\n 2 Audi         4 cyl            76\n 3 Audi         5 cyl             9\n 4 Audi         6 cyl            47\n 5 Audi         8 cyl            39\n 6 Audi         10 cyl            3\n 7 Audi         12 cyl           18\n 8 Audi         16 cyl            0\n 9 BMW          3 cyl            11\n10 BMW          4 cyl            78\n# ℹ 30 more rows\n\n\nIn the function call, you need to specify the following: if you were not to use the pipe, the first argument would be the tibble you are manipulating. Then, you look at the column you want to keep. Here, it is the car manufacturer. This means that all columns but manufacturer will be crammed into two new ones: one will contain the columns’ names, the other one their values. How are those new column supposed to be named? That can be specified in the names_to = and values_to =arguments. Please note that you need to provide them a character vector, hence, surround your parameters with quotation marks. As a rule of thumb for all tidyverse packages: If it is a new column name you provide, surround it with quotation marks. If it is one that already exists – like, here, manufacturer, then you do not need the quotation marks.\n\n\nVariables in both rows and columns\nYou have this data set:\n\ncar_models_fuel &lt;- tribble(\n  ~manufacturer, ~model, ~cylinders, ~fuel_consumption_type, ~fuel_consumption_per_100km,\n  \"VW\", \"Golf\", 4, \"urban\", 5.2,\n  \"VW\", \"Golf\", 4, \"extra urban\", 4.5,\n  \"Opel\", \"Adam\", 4, \"urban\", 4.9,\n  \"Opel\", \"Adam\", 4, \"extra urban\", 4.1\n  )\ncar_models_fuel\n\n# A tibble: 4 × 5\n  manufacturer model cylinders fuel_consumption_type fuel_consumption_per_100km\n  &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;                                      &lt;dbl&gt;\n1 VW           Golf          4 urban                                        5.2\n2 VW           Golf          4 extra urban                                  4.5\n3 Opel         Adam          4 urban                                        4.9\n4 Opel         Adam          4 extra urban                                  4.1\n\n\nIt was created using the tribble function: tibbles can also be created by row. First, the column names need to be specified by putting a tilde (~) in front of them. Then, you can put in values separated by commas. Please note that the number of values needs to be a multiple of the number of columns.\nIn this data set, there are basically five variables: manufacturer, model, cylinders, urban fuel consumption, and extra urban fuel consumption. However, the column fuel_consumption_type does not store a variable but the names of two variables. Hence, you need to fix this to make the data set tidy. Because this encompasses reducing the number of rows, the data set becomes wider. The function to achieve this is therefore called pivot_wider() and the inverse of pivot_longer().\n\ncar_models_fuel_tidy &lt;- car_models_fuel |&gt; \n  pivot_wider(\n    names_from = fuel_consumption_type, \n    values_from = fuel_consumption_per_100km\n    )\n\ncar_models_fuel_tidy\n\n# A tibble: 2 × 5\n  manufacturer model cylinders urban `extra urban`\n  &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 VW           Golf          4   5.2           4.5\n2 Opel         Adam          4   4.9           4.1\n\n\nHere, you only need to specify the columns you fetch the names and values from. As they both do already exist, you do not need to wrap them in quotation marks.\n\n\nMultiple variables in one column\nNow, however, there is a problem with the cylinders: their number should be depicted in a numeric vector. We could achieve this by either parsing it to a numeric vector:\n\nger_car_manufacturer_longer$cylinders &lt;- parse_number(ger_car_manufacturer_longer$cylinders)\n\nOn the other hand, we can also use a handy function from tidyr called separate() and afterwards drop the unnecessary column:\n\nger_car_manufacturer_longer_sep_cyl &lt;- ger_car_manufacturer_longer |&gt; # first, take the tibble\n  separate(cylinders, into = c(\"cylinders\", \"drop_it\"), sep = \" \") |&gt; # and then split the column \"cylinders\" into two\n  select(-drop_it) # you will learn about this in the lesson on dplyr  # and then drop one column from the tibble\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 40 rows [1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\n\nIf there are two (or actually more) relevant values in one column, you can simply let out the dropping process and easily split them into multiple columns. By default, the sep = argument divides the content by all non-alphanumeric characters (every character that is not a letter, number, or space) it contains.\nPlease note that the new column is still in character format. We can change this using as.numeric():\n\nger_car_manufacturer_longer_sep_cyl$cylinders &lt;- as.numeric(ger_car_manufacturer_longer_sep_cyl$cylinders)\n\nFurthermore, you might want to sort your data in a different manner. If you want to do this by cylinders, it would look like this:\n\narrange(ger_car_manufacturer_longer_sep_cyl, cylinders)\n\n# A tibble: 40 × 3\n   manufacturer cylinders frequency\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 Audi                 3         6\n 2 BMW                  3        11\n 3 Mercedes             3         3\n 4 Opel                 3        16\n 5 VW                   3         7\n 6 Audi                 4        76\n 7 BMW                  4        78\n 8 Mercedes             4        84\n 9 Opel                 4        56\n10 VW                   4        70\n# ℹ 30 more rows\n\n\n\n\n\nInsertion: the pipe\nHave you noticed the |&gt;? That’s the pipe. It can be considered a conjunction in coding. Usually, you will use it when working with tibbles. What it does is pretty straight-forward: it takes what is on its left – the input – and provides it to the function on its right as the first argument. Hence, the code in the last chunk, which looks like this\n\narrange(ger_car_manufacturer_longer_sep_cyl, cylinders)\n\n# A tibble: 40 × 3\n   manufacturer cylinders frequency\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 Audi                 3         6\n 2 BMW                  3        11\n 3 Mercedes             3         3\n 4 Opel                 3        16\n 5 VW                   3         7\n 6 Audi                 4        76\n 7 BMW                  4        78\n 8 Mercedes             4        84\n 9 Opel                 4        56\n10 VW                   4        70\n# ℹ 30 more rows\n\n\ncould have also been written like this\n\nger_car_manufacturer_longer_sep_cyl |&gt; arrange(cylinders)\n\n# A tibble: 40 × 3\n   manufacturer cylinders frequency\n   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 Audi                 3         6\n 2 BMW                  3        11\n 3 Mercedes             3         3\n 4 Opel                 3        16\n 5 VW                   3         7\n 6 Audi                 4        76\n 7 BMW                  4        78\n 8 Mercedes             4        84\n 9 Opel                 4        56\n10 VW                   4        70\n# ℹ 30 more rows\n\n\nbecause the tibble is the first argument in the function call.\nBecause the pipe (its precedessor was %&gt;%) has really gained traction in the R community, many functions are now optimized for being used with the pipe. However, there are still some around which are not. A function for fitting a basic linear model with one dependent and one independent variable which are both stored in a tibble looks like this: lm(formula = dv ~ iv, data = tibble). Here, the tibble is not the first argument. To be able to fit a linear model in a “pipeline,” you need to employ a little hack: you can use an underscore _ as a placeholder. Here, it is important that the argument is named.\nLet’s check out the effect the number of cylinders has on the number of models:\n\nger_car_manufacturer_longer_sep_cyl |&gt; \n  lm(frequency ~ cylinders, data = _) |&gt; \n  summary()\n\n\nCall:\nlm(formula = frequency ~ cylinders, data = ger_car_manufacturer_longer_sep_cyl)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.211 -15.512  -0.433  14.300  50.541 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  44.4674     7.4953   5.933 7.04e-07 ***\ncylinders    -2.7522     0.8315  -3.310  0.00205 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.84 on 38 degrees of freedom\nMultiple R-squared:  0.2238,    Adjusted R-squared:  0.2033 \nF-statistic: 10.95 on 1 and 38 DF,  p-value: 0.002052\n\n\nAs |&gt; is a bit tedious to type, a shortcut exists: shift-ctrl-m.\n\n\nSplitting and merging cells\nIf there are multiple values in one column/cell and you want to split them and put them into two rows instead of columns, tidyr offers you the separate_rows() function.\n\ngerman_cars_vec &lt;- c(Audi = \"A1, A3, A4, A5, A6, A7, A8\", \n                     BMW = \"1 Series, 2 Series, 3 Series, 4 Series, 5 Series, 6 Series, 7 Series, 8 Series\")\ngerman_cars_tbl &lt;- enframe(\n  german_cars_vec, \n  name = \"brand\", \n  value = \"model\"\n  )\n\ngerman_cars_tbl\n\n# A tibble: 2 × 2\n  brand model                                                                   \n  &lt;chr&gt; &lt;chr&gt;                                                                   \n1 Audi  A1, A3, A4, A5, A6, A7, A8                                              \n2 BMW   1 Series, 2 Series, 3 Series, 4 Series, 5 Series, 6 Series, 7 Series, 8…\n\ntidy_german_cars_tbl &lt;- german_cars_tbl |&gt; \n  separate_rows(model, sep = \", \")\n\nenframe() enables you to create a tibble from a (named) vector. It outputs a tibble with two columns (name and value by default): name contains the names of the elements (if the elements are unnamed, it contains a serial number), value the element. Both can be renamed in the function call by providing a character vector.\nIf you want to achieve the opposite, i.e., merge cells’ content, you can use the counterpart, unite(). Let’s take the following dataframe which consists of the names of the professors of the Institute for Political Science of the University of Regensburg:\n\nprofessor_names_df &lt;- data.frame(first_name = c(\"Karlfriedrich\", \"Martin\", \"Jerzy\", \"Stephan\", \"Melanie\"),\n                                 last_name = c(\"Herb\", \"Sebaldt\", \"Maćków\", \"Bierling\", \"Walter-Rogg\"))\n\nprofessor_names_tbl &lt;- professor_names_df |&gt; \n  as_tibble() |&gt; \n  unite(first_name, last_name, col = \"name\", sep = \" \", remove = TRUE, na.rm = FALSE)\n\nprofessor_names_tbl\n\n# A tibble: 5 × 1\n  name               \n  &lt;chr&gt;              \n1 Karlfriedrich Herb \n2 Martin Sebaldt     \n3 Jerzy Maćków       \n4 Stephan Bierling   \n5 Melanie Walter-Rogg\n\n\nunite() takes the tibble it should be applied to as the first argument (not necessary if you use the pipe). Then, it takes the two or more columns as arguments (actually, this is not necessary if you want to unite all columns). col = takes a character vector to specify the name of the resulting, new column. remove = TRUE indicates that the columns that are united are removed as well. You can, of course, set it to false, too. na.rm = FALSE finally indicates that missing values are not to be removed prior to the uniting process.\nHere, the final variant of creating tibbles is introduced as well: you can apply the function as_tibble() to a data frame and it will then be transformed into a tibble.\n\n\nFurther links\n\nHadley on tidy data\n\nThe two pivot_*() functions lie at the heart of tidyr. This article from the Northeastern University’s School of Journalism explains it in further detail.\n\n\n\nExercises\nBring the data sets you read into R in the “Reading data in R” section into a tidy format. Store the tidy data sets in a new object, named like the former object plus the suffix “_tidy” – e.g., books_tidy. If no tidying is needed, you do not have to create a new object. The pipe operator should be used to connect the different steps."
  },
  {
    "objectID": "2_r_catch-up.html#wrangling-data-with-dplyr",
    "href": "2_r_catch-up.html#wrangling-data-with-dplyr",
    "title": "Chapter 2: Brief R Recap",
    "section": "Wrangling data with dplyr",
    "text": "Wrangling data with dplyr\nThe last chapter showed you four things: how you get data sets into R, a couple of ways to create tibbles, how to pass data to functions using the pipe (|&gt;), and an introduction to tidy data and how to make data sets tidy using the tidyr package (Wickham 2020b). What you haven’t learned was how you can actually manipulate the data themselves. In the tidyverse framework (Wickham et al. 2019), the package which enables you to accomplish those tasks is dplyr (Wickham 2020a).\ndplyr joined the party in 2014, building upon the plyr package. The d in dplyr stands for data set and dplyr works with tibbles (or data frames) only.\nIt consists of five main functions, the “verbs”:\n\narrange() – sort values\nfilter() – pick observations\nmutate() – create new variables (columns)\nselect() – select variables\nsummarize() – create summaries from multiple values\n\nThey are joined by group_by(), a function that changes the scope on which entities the functions are applied to.\nFurthermore, diverse bind_ functions and _joins enable you to combine multiple tibbles into one. They will be introduced later.\nIn the following, I will guide you through how you can use the verbs to accomplish whatever goals which require data wrangling you might have.\nThe data set I will use here consists of the 1,000 most popular movies on IMDb which were published between 2006 and 2016 and some data on them. It was created by PromptCloud and DataStock and published on Kaggle, more information can be found here.\n\nimdb_raw &lt;- read_csv(\"https://www.dropbox.com/s/wfwyxjkpo24e3yq/imdb2006-2016.csv?dl=1\")\n\nThe data set hasn’t been modified by me before. I will show you how I would go across it using a couple of dplyr functions.\n\nselect()\nselect enables you to select columns. Since we are dealing with tidy data, every variable has its own column.\nglimpse() provides you with an overview of the data set and its columns.\n\nglimpse(imdb_raw)\n\nRows: 1,000\nColumns: 12\n$ Rank                 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ Title                &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\",…\n$ Genre                &lt;chr&gt; \"Action,Adventure,Sci-Fi\", \"Adventure,Mystery,Sci…\n$ Description          &lt;chr&gt; \"A group of intergalactic criminals are forced to…\n$ Director             &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan…\n$ Actors               &lt;chr&gt; \"Chris Pratt, Vin Diesel, Bradley Cooper, Zoe Sal…\n$ Year                 &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2…\n$ `Runtime (Minutes)`  &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, …\n$ Rating               &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0,…\n$ Votes                &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258…\n$ `Revenue (Millions)` &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 15…\n$ Metascore            &lt;dbl&gt; 76, 65, 62, 59, 40, 42, 93, 71, 78, 41, 66, 74, 6…\n\n\nThe columns I want to keep are: Title, Director, Year, Runtime (Minutes), Rating, Votes, and Revenue (Millions). Furthermore, I want to rename the columns: every column’s name should be in lowercase and a regular name that does not need to be surrounded by back ticks – i.e., a name that only consists of characters, numbers, underscores, or dots.\nThis can be achieved in a couple of ways:\nFirst, by choosing the columns column by column and subsequently renaming them:\n\nimdb_raw |&gt; \n  select(Title, Director, Year, `Runtime (Minutes)`, Rating, Votes, `Revenue (Millions)`) |&gt; \n  rename(title = Title, director = Director, year = Year, runtime = `Runtime (Minutes)`, rating = Rating, votes = Votes, revenue_million = `Revenue (Millions)`) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\nSecond, the columns can also be chosen vice versa: unnecessary columns can be dropped using a minus:\n\nimdb_raw |&gt; \n  select(-Rank, -Genre, -Description, -Actors, -Metascore) |&gt; \n  rename(title = Title, director = Director, year = Year, runtime = `Runtime (Minutes)`, rating = Rating, votes = Votes, revenue_million = `Revenue (Millions)`) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\nColumns can also be renamed in the selecting process:\n\nimdb_raw |&gt; \n  select(title = Title, director = Director, year = Year, runtime = `Runtime (Minutes)`, rating = Rating, votes = Votes, revenue_million = `Revenue (Millions)`) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\nYou can also make your expressions shorter by using a couple of hacks:\n: can be used to select all columns between two:\n\nimdb_raw |&gt; \n  select(Title, Director, Year:`Revenue (Millions)`) |&gt; \n  rename(title = Title, director = Director, year = Year, runtime = `Runtime (Minutes)`, rating = Rating, votes = Votes, revenue_million = `Revenue (Millions)`) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\nstarts_with() select columns whose names start with the same character string:\n\nimdb_selected &lt;- imdb_raw |&gt; \n  select(Title, Director, Votes, Year, starts_with(\"R\")) |&gt; \n  select(-Rank) |&gt; \n  rename(title = Title, director = Director, year = Year, runtime = `Runtime (Minutes)`, rating = Rating, votes = Votes, revenue_million = `Revenue (Millions)`) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\nAs you may have noticed, the order in the select() matters: columns will be ordered in the same order as they are chosen.\nA couple of further shortcuts for select() do exist. An overview can be found in the dplyr cheatsheet.\n\n\nfilter()\nWhereas select() enables you to choose variables (i.e., columns), filter() lets you choose observations (i.e., rows).\nIn this case, I only want movies with a revenue above $100,000,000:\n\nimdb_selected |&gt; \n  filter(revenue_million &gt; 100) |&gt; \n  glimpse()\n\nRows: 250\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 258682, 192177,…\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 128, 116, 133, 127, 133, 107,…\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 8.3, 7.0, 7.5, 7.8, 7.9, 7.7,…\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 151.06, 100.01…\n\n\nBesides, I am especially interested in the director Christopher Nolan. Therefore, I want to look at movies that were directed by him and made more than $100,000,000:\n\nimdb_selected |&gt; \n  filter(revenue_million &gt; 100 & director == \"Christopher Nolan\") |&gt; \n  glimpse()\n\nRows: 4\nColumns: 7\n$ title           &lt;chr&gt; \"Interstellar\", \"The Dark Knight\", \"Inception\", \"The D…\n$ director        &lt;chr&gt; \"Christopher Nolan\", \"Christopher Nolan\", \"Christopher…\n$ votes           &lt;dbl&gt; 1047747, 1791916, 1583625, 1222645\n$ year            &lt;dbl&gt; 2014, 2008, 2010, 2012\n$ runtime         &lt;dbl&gt; 169, 152, 148, 164\n$ rating          &lt;dbl&gt; 8.6, 9.0, 8.8, 8.5\n$ revenue_million &lt;dbl&gt; 187.99, 533.32, 292.57, 448.13\n\n\nThe following overview is taken from the dplyr cheatsheet and shows the operators you can use in filter():\n\n\n\nOverview of comparison operators\n\n\n\nExemplary application\nTo demonstrate how a real-world application of this stuff could look like, I will now provide you a brief insight into my private life and how I organize movie nights. JK. You could definitely try this at home and surprise your loved ones with such hot applications. If you are brave and surprise your latest Tinder match with an .RDS file containing suggestions for Netflix&Chill, please let me know what their response looked like.\nTonight, I will hang out with a real nerd. Probably because they (nerds have all kinds of genders) know about my faible for R, they have sent me a vector containing a couple of movies we could watch tonight:\n\nset.seed(123) # guarantees that movie_vec will always be the same thing\nmovie_vec &lt;- imdb_raw$Title[sample(1000, 10, replace = FALSE)]\nmovie_vec\n\n [1] \"Mechanic: Resurrection\" \"Denial\"                 \"The Conjuring 2\"       \n [4] \"Birth of the Dragon\"    \"Warrior\"                \"Super\"                 \n [7] \"127 Hours\"              \"Dangal\"                 \"The Infiltrator\"       \n[10] \"Maleficent\"            \n\n\nHowever, I want to make a more informed decision and decide to obtain some more information on the movies from my IMDb data set:\n\nimdb_selected |&gt; \n  filter(title %in% movie_vec) |&gt; \n  glimpse()\n\nRows: 10\nColumns: 7\n$ title           &lt;chr&gt; \"Dangal\", \"The Conjuring 2\", \"Warrior\", \"Maleficent\", …\n$ director        &lt;chr&gt; \"Nitesh Tiwari\", \"James Wan\", \"Gavin O'Connor\", \"Rober…\n$ votes           &lt;dbl&gt; 48969, 137203, 355722, 268877, 43929, 48161, 8229, 552…\n$ year            &lt;dbl&gt; 2016, 2016, 2011, 2014, 2016, 2016, 2016, 2016, 2010, …\n$ runtime         &lt;dbl&gt; 161, 134, 140, 97, 127, 98, 109, 103, 94, 96\n$ rating          &lt;dbl&gt; 8.8, 7.4, 8.2, 7.0, 7.1, 5.6, 6.6, 3.9, 7.6, 6.8\n$ revenue_million &lt;dbl&gt; 11.15, 102.46, 13.65, 241.41, 15.43, 21.20, 4.07, 93.0…\n\n\nI have convinced them to watch either one of the movies they have suggested or one directed by Christopher Nolan or one with a rating greater or equal to 8.5 and send them back this data set:\n\nimdb_selected |&gt; \n  filter(title %in% movie_vec | director == \"Christopher Nolan\" | rating &gt;= 8.5) |&gt; \n  glimpse()\n\nRows: 21\nColumns: 7\n$ title           &lt;chr&gt; \"Interstellar\", \"The Dark Knight\", \"The Prestige\", \"In…\n$ director        &lt;chr&gt; \"Christopher Nolan\", \"Christopher Nolan\", \"Christopher…\n$ votes           &lt;dbl&gt; 1047747, 1791916, 913152, 1583625, 34110, 937414, 4896…\n$ year            &lt;dbl&gt; 2014, 2008, 2006, 2010, 2016, 2006, 2016, 2012, 2014, …\n$ runtime         &lt;dbl&gt; 169, 152, 130, 148, 106, 151, 161, 164, 107, 134, 140,…\n$ rating          &lt;dbl&gt; 8.6, 9.0, 8.5, 8.8, 8.6, 8.5, 8.8, 8.5, 8.5, 7.4, 8.2,…\n$ revenue_million &lt;dbl&gt; 187.99, 533.32, 53.08, 292.57, 4.68, 132.37, 11.15, 44…\n\n\n“I deteste ‘Interstellar’,” is the response. “All right,” I say to myself, “I can easily exclude it.”\n\nimdb_selected |&gt; \n  filter(title %in% movie_vec | director == \"Christopher Nolan\" | rating &gt;= 8.5 & title != \"Interstellar\") |&gt; # if you want to negate something, put the ! in front of it\n  glimpse()\n\nRows: 21\nColumns: 7\n$ title           &lt;chr&gt; \"Interstellar\", \"The Dark Knight\", \"The Prestige\", \"In…\n$ director        &lt;chr&gt; \"Christopher Nolan\", \"Christopher Nolan\", \"Christopher…\n$ votes           &lt;dbl&gt; 1047747, 1791916, 913152, 1583625, 34110, 937414, 4896…\n$ year            &lt;dbl&gt; 2014, 2008, 2006, 2010, 2016, 2006, 2016, 2012, 2014, …\n$ runtime         &lt;dbl&gt; 169, 152, 130, 148, 106, 151, 161, 164, 107, 134, 140,…\n$ rating          &lt;dbl&gt; 8.6, 9.0, 8.5, 8.8, 8.6, 8.5, 8.8, 8.5, 8.5, 7.4, 8.2,…\n$ revenue_million &lt;dbl&gt; 187.99, 533.32, 53.08, 292.57, 4.68, 132.37, 11.15, 44…\n\n\nOh, that did not work. I should wrap them in columns:\n\nimdb_selected |&gt; \n  filter((title %in% movie_vec | director == \"Christopher Nolan\" | rating &gt;= 8.5) & title != \"Interstellar\") |&gt; \n  glimpse()\n\nRows: 20\nColumns: 7\n$ title           &lt;chr&gt; \"The Dark Knight\", \"The Prestige\", \"Inception\", \"Kimi …\n$ director        &lt;chr&gt; \"Christopher Nolan\", \"Christopher Nolan\", \"Christopher…\n$ votes           &lt;dbl&gt; 1791916, 913152, 1583625, 34110, 937414, 48969, 122264…\n$ year            &lt;dbl&gt; 2008, 2006, 2010, 2016, 2006, 2016, 2012, 2014, 2016, …\n$ runtime         &lt;dbl&gt; 152, 130, 148, 106, 151, 161, 164, 107, 134, 140, 97, …\n$ rating          &lt;dbl&gt; 9.0, 8.5, 8.8, 8.6, 8.5, 8.8, 8.5, 8.5, 7.4, 8.2, 7.0,…\n$ revenue_million &lt;dbl&gt; 533.32, 53.08, 292.57, 4.68, 132.37, 11.15, 448.13, 13…\n\n\nThey come up with a new idea: we have a Scottish evening with a movie directed by the Scottish director Gillies MacKinnon:\n\nimdb_selected |&gt; \n  filter(director == \"Gillies MacKinnon\") |&gt; \n  glimpse()\n\nRows: 1\nColumns: 7\n$ title           &lt;chr&gt; \"Whisky Galore\"\n$ director        &lt;chr&gt; \"Gillies MacKinnon\"\n$ votes           &lt;dbl&gt; 102\n$ year            &lt;dbl&gt; 2016\n$ runtime         &lt;dbl&gt; 98\n$ rating          &lt;dbl&gt; 5\n$ revenue_million &lt;dbl&gt; NA\n\n\n“Well, apparently there is a problem in the data set,” I notice. “There is an NA in the revenue column. I should probably have a further look at this.”\n\nimdb_selected |&gt; \n  filter(is.na(revenue_million)) |&gt; \n  glimpse()\n\nRows: 128\nColumns: 7\n$ title           &lt;chr&gt; \"Mindhorn\", \"Hounds of Love\", \"Paris pieds nus\", \"5- 2…\n$ director        &lt;chr&gt; \"Sean Foley\", \"Ben Young\", \"Dominique Abel\", \"Patrick …\n$ votes           &lt;dbl&gt; 2490, 1115, 222, 241, 496, 5103, 987, 35870, 149791, 7…\n$ year            &lt;dbl&gt; 2016, 2016, 2016, 2007, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 89, 108, 83, 113, 73, 91, 130, 86, 133, 106, 105, 118,…\n$ rating          &lt;dbl&gt; 6.4, 6.7, 6.8, 7.1, 2.7, 5.6, 3.7, 6.8, 5.9, 7.9, 5.8,…\n$ revenue_million &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWell, that’s quite a significant number of NAs. I will need to exclude these cases:\n\nimdb_selected |&gt; \n  filter(!is.na(revenue_million)) |&gt; \n  glimpse()\n\nRows: 872\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 141, 116, 133, 127,…\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 7.1, 7.0, 7.5, 7.8,…\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\n\n\nOther possibilities to subset observations\nslice() selects rows by positions:\n\nimdb_selected |&gt; \n  slice(1:10) |&gt; \n  glimpse()\n\nRows: 10\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\n\nimdb_selected |&gt; \n  slice_min(revenue_million, n = 10) |&gt; \n  glimpse()\n\nRows: 10\nColumns: 7\n$ title           &lt;chr&gt; \"A Kind of Murder\", \"Dead Awake\", \"Wakefield\", \"Loveso…\n$ director        &lt;chr&gt; \"Andy Goddard\", \"Phillip Guzman\", \"Robin Swicord\", \"So…\n$ votes           &lt;dbl&gt; 3305, 523, 291, 616, 80415, 10220, 36091, 54027, 4155,…\n$ year            &lt;dbl&gt; 2016, 2016, 2016, 2016, 2014, 2015, 2010, 2012, 2015, …\n$ runtime         &lt;dbl&gt; 95, 99, 106, 84, 102, 101, 98, 95, 93, 110\n$ rating          &lt;dbl&gt; 5.2, 4.7, 7.5, 6.4, 7.2, 5.9, 6.5, 6.9, 5.6, 5.9\n$ revenue_million &lt;dbl&gt; 0.00, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, …\n\n\ndistinct removes duplicate rows:\n\nimdb_selected |&gt; \n  distinct(director) |&gt; \n  glimpse()\n\nRows: 644\nColumns: 1\n$ director &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"Christop…\n\n\nBy default, it will remove all other columns apart from the one(s) you have specified. You can avoid that by setting .keep_all = TRUE:\n\nimdb_selected |&gt; \n  distinct(title, .keep_all = TRUE) |&gt; \n  glimpse()\n\nRows: 999\nColumns: 7\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n\n\nOh, interesting, there is apparently one movie which is in there twice. How could we find this movie?\n\n\n\nmutate()\nMy data set looks pretty nice already, but one flaw catches the eye: the column revenue_million should probably be converted to revenue. Hence, I need to create a new variable which contains the values from revenue_million multiplied by 1,000,000 and drop the now obsolete revenue_million.\n\nimdb_selected |&gt; \n  mutate(revenue = revenue_million * 1000000) |&gt; \n  select(-revenue_million) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 7\n$ title    &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sing\", \"Su…\n$ director &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"Christop…\n$ votes    &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, 2490, 7…\n$ year     &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2…\n$ runtime  &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, 127, 13…\n$ rating   &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5, 7.8, 7…\n$ revenue  &lt;dbl&gt; 333130000, 126460000, 138120000, 270320000, 325020000, 451300…\n\n\nThe structure of the mutate() call looks like this: first, you need to provide the name of the new variable. If the variable exists already, it will be replaced. Second, the equal sign tells R what the new variable should contain. Third, a function that outputs a vector which is as long as the tibble has rows or 1.\nIf we want to drop all other columns and just keep the new one: transmute() drops all the original columns.\n\nimdb_selected |&gt; \n  transmute(revenue = revenue_million * 1000000) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 1\n$ revenue &lt;dbl&gt; 333130000, 126460000, 138120000, 270320000, 325020000, 4513000…\n\n\nmutate() uses so-called window functions. They take one vector of values and return another vector of values. An overview – again, from the cheat sheet:\n\n\n\nWindow functions\n\n\nAnother feature of dplyr, which is useful in combination with mutate(), is case_when().\ncase_when() can for instance be used to create binary indicator variables. In this example I want it to be 0 if the movie was made before 2010 and 1 if not. case_when() works like this: first, you provide a condition (e.g., year &lt; 2010). Second, you provide what the output should be if the condition is met (here, 0). Third, you can provide as many conditions as you want. Finally, you can provide a default value using TRUE ~ value. If none of the conditions are met, the default value will be assigned.\n\nimdb_selected |&gt; \n  mutate(indicator = case_when(year &lt; 2010 ~ 0,\n                               year &gt;= 2010 ~ 1,\n                               TRUE ~ 2)) |&gt; \n  glimpse()\n\nRows: 1,000\nColumns: 8\n$ title           &lt;chr&gt; \"Guardians of the Galaxy\", \"Prometheus\", \"Split\", \"Sin…\n$ director        &lt;chr&gt; \"James Gunn\", \"Ridley Scott\", \"M. Night Shyamalan\", \"C…\n$ votes           &lt;dbl&gt; 757074, 485820, 157606, 60545, 393727, 56036, 258682, …\n$ year            &lt;dbl&gt; 2014, 2012, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ runtime         &lt;dbl&gt; 121, 124, 117, 108, 123, 103, 128, 89, 141, 116, 133, …\n$ rating          &lt;dbl&gt; 8.1, 7.0, 7.3, 7.2, 6.2, 6.1, 8.3, 6.4, 7.1, 7.0, 7.5,…\n$ revenue_million &lt;dbl&gt; 333.13, 126.46, 138.12, 270.32, 325.02, 45.13, 151.06,…\n$ indicator       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\nKeep in mind that you can throw any function into mutate() as long as it is vectorized and the output has the same length as the tibble or 1.\ncase_when() also has a sibling called case_match(). It is used when you want to create a new variable based on the values of a categorical variable. It works similar to case_when(), but instead of providing conditions, you provide the exact values you want to match. case_match() provides a cleaner syntax when you’re matching exact values. It’s particularly useful when you want to recode or map specific values to new ones.\n\nimdb_selected |&gt; \n  mutate(\n    # Match specific years to decades\n    decade = case_match(\n      year,\n      2006:2009 ~ \"2000s\",\n      2010:2016 ~ \"2010s\",\n      .default = \"Unknown\"\n    )\n  ) |&gt; \n  count(decade)\n\n# A tibble: 2 × 2\n  decade     n\n  &lt;chr&gt;  &lt;int&gt;\n1 2000s    200\n2 2010s    800\n\n\nYou can also use case_match() with character values:\n\nimdb_selected |&gt; \n  mutate(\n    director_type = case_match(\n      director,\n      c(\"Christopher Nolan\", \"Steven Spielberg\", \"Martin Scorsese\") ~ \"Famous\",\n      .default = \"Other\"\n    )\n  ) |&gt; \n  count(director_type)\n\n# A tibble: 2 × 2\n  director_type     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Famous           14\n2 Other           986\n\n\nKey differences between case_when() and case_match():\n\nSyntax: case_match() uses the value to match on the left side, while case_when() uses conditions\nUse case: case_match() is for exact matching, case_when() is for complex conditions\nPerformance: case_match() can be faster for simple value matching\nReadability: case_match() is often cleaner when recoding variables\n\n\n\nsummarize(), group_by(), and reframe()\nWhen you analyze data, you often want to compare entities according to some sort of summary statistic. This means that you, first, need to split up your data set into certain groups which share one or more characteristics, and, second, collapse the rows together into single-row summaries. The former challenge is accomplished using group_by() whose argument is one or more variables, the latter requires the summarize() function. This function works similar to mutate() but uses summary functions – which take a vector of multiple values and return a single value – instead of window functions – which return a vector of the same length as the input.\nLet me provide you an example.\nI am interested in the director’s average ratings:\n\nimdb_selected |&gt; \n  group_by(director, year) |&gt; \n  summarize(avg_rating = mean(rating),\n            avg_revenue = mean(revenue_million, na.rm = TRUE))\n\n`summarise()` has grouped output by 'director'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 987 × 4\n# Groups:   director [644]\n   director             year avg_rating avg_revenue\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 Aamir Khan           2007        8.5        1.2 \n 2 Abdellatif Kechiche  2013        7.8        2.2 \n 3 Adam Leon            2016        6.5      NaN   \n 4 Adam McKay           2006        6.6      148.  \n 5 Adam McKay           2008        6.9      100.  \n 6 Adam McKay           2010        6.7      119.  \n 7 Adam McKay           2015        7.8       70.2 \n 8 Adam Shankman        2007        6.7      119.  \n 9 Adam Shankman        2012        5.9       38.5 \n10 Adam Wingard         2014        6.7        0.32\n# ℹ 977 more rows\n\n\nIn general, summarize() always works like this: first, you change the scope from the entire tibble to different groups. Then, you calculate your summary. If you then want to further manipulate your data or calculate something else based on the new summary, you need to call ungroup().\nYou can see the summary functions below:\n\n\n\nSummary functions in R\n\n\nAnother handy function akin to this is count(). It counts all occurrences of a singular value in the tibble.\nIf I were interested in how many movies of the different directors have made it into the data set, I could use this code:\n\nimdb_selected |&gt; \n  count(director)\n\n# A tibble: 644 × 2\n   director                n\n   &lt;chr&gt;               &lt;int&gt;\n 1 Aamir Khan              1\n 2 Abdellatif Kechiche     1\n 3 Adam Leon               1\n 4 Adam McKay              4\n 5 Adam Shankman           2\n 6 Adam Wingard            2\n 7 Afonso Poyart           1\n 8 Aisling Walsh           1\n 9 Akan Satayev            1\n10 Akiva Schaffer          1\n# ℹ 634 more rows\n\n\nWhile summarize() is powerful, it has a limitation: it always returns exactly one row per group. Sometimes you need more flexibility - that’s where reframe() comes in. Introduced in dplyr 1.1.0, reframe() allows you to return any number of rows per group.\nWith reframe(), we can for instance calculate the rating quantiles per director:\n\nimdb_selected |&gt; \n  group_by(director) |&gt; \n  reframe(\n    rating_quantiles = quantile(rating, probs = c(0.25, 0.5, 0.75)),\n    quantile = rep(c(0.25, 0.5, 0.75))\n  ) |&gt; \n    ungroup()\n\n# A tibble: 1,932 × 3\n   director            rating_quantiles quantile\n   &lt;chr&gt;                          &lt;dbl&gt;    &lt;dbl&gt;\n 1 Aamir Khan                      8.5      0.25\n 2 Aamir Khan                      8.5      0.5 \n 3 Aamir Khan                      8.5      0.75\n 4 Abdellatif Kechiche             7.8      0.25\n 5 Abdellatif Kechiche             7.8      0.5 \n 6 Abdellatif Kechiche             7.8      0.75\n 7 Adam Leon                       6.5      0.25\n 8 Adam Leon                       6.5      0.5 \n 9 Adam Leon                       6.5      0.75\n10 Adam McKay                      6.68     0.25\n# ℹ 1,922 more rows\n\n\nThis example calculates the 25th, 50th, and 75th percentiles of ratings. Each director will have one row with their average rating and a list of quantiles.\nWhen to use reframe() vs summarize():\nUse summarize() when you want:\n\nOne summary value per group (mean, sum, count, etc.)\nA single row of results per group\n\nUse reframe() when you need:\n\nMultiple rows per group\nTo return quantiles, ranges, or other multi-value summaries\nMore flexibility in your output structure\n\nNote that both functions return a grouped tibble, so you may want to ungroup() afterwards if you’re doing further operations.\n\n\narrange()\nFinally, you can also sort values using arrange(). In the last section, I was interested in directors’ respective average ratings. The values were ordered according to their name (hence, “Aamir Khan” was first). In this case, the order dos not make too much sense, because the first name does not say too much about the director’s ratings. Therefore, I want to sort them according to their average ratings:\n\nimdb_selected |&gt; \n  group_by(director) |&gt; \n  summarize(avg_rating = mean(rating)) |&gt; \n  arrange(avg_rating)\n\n# A tibble: 644 × 2\n   director           avg_rating\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 Jason Friedberg           1.9\n 2 James Wong                2.7\n 3 Shawn Burkett             2.7\n 4 Jonathan Holbrook         3.2\n 5 Femi Oyeniran             3.5\n 6 Micheal Bafaro            3.5\n 7 Jeffrey G. Hunt           3.7\n 8 Rolfe Kanefsky            3.9\n 9 Joey Curtis               4  \n10 Sam Taylor-Johnson        4.1\n# ℹ 634 more rows\n\n\nAll right, Jason Friedberg is apparently the director of the worst rated movie in my data set. But it would be more handy, if they were arranged in descending order. I can use desc() for this:\n\nimdb_selected |&gt; \n  group_by(director) |&gt; \n  summarize(avg_rating = mean(rating)) |&gt; \n  arrange(-avg_rating)\n\n# A tibble: 644 × 2\n   director                         avg_rating\n   &lt;chr&gt;                                 &lt;dbl&gt;\n 1 Nitesh Tiwari                          8.8 \n 2 Christopher Nolan                      8.68\n 3 Makoto Shinkai                         8.6 \n 4 Olivier Nakache                        8.6 \n 5 Aamir Khan                             8.5 \n 6 Florian Henckel von Donnersmarck       8.5 \n 7 Damien Chazelle                        8.4 \n 8 Naoko Yamada                           8.4 \n 9 Amber Tamblyn                          8.3 \n10 Lee Unkrich                            8.3 \n# ℹ 634 more rows\n\n\nChapeau, Nitesh Tiwari!\n\n\nIntroducing joins\nThe last session showed you three things: how you get data sets into R, a couple of ways to create tibbles, and an introduction to tidy data and how to make data sets tidy using the tidyr package. As you may recall from the last session, it was not able to solve the last two problems with only the tools tidyr offers. In particular, the problems were:\n\nMultiple types of observational units are stored in the same table.\nA single observational unit is stored in multiple tables.\n\nBoth problems need some different kind of tools: joins. Joins can be used to merge tibbles together. This tutorial, again, builds heavily on the R for Data Science book (Wickham and Grolemund 2016)\n\nMultiple types of units are in the same table\nLet’s look at the following data set. It contains the billboard charts in 2000 and was obtained from the tidyr GitHub repo. The example below is taken from the tidyr vignette which can be loaded using vignette(\"tidy-data\", package = \"tidyr\").\n\nload(\"data/billboard.rda\")\n\n\nglimpse(billboard)\n\nRows: 317\nColumns: 79\n$ artist       &lt;chr&gt; \"2 Pac\", \"2Ge+her\", \"3 Doors Down\", \"3 Doors Down\", \"504 …\n$ track        &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"The Hardest Part Of ...\", \"Kr…\n$ date.entered &lt;date&gt; 2000-02-26, 2000-09-02, 2000-04-08, 2000-10-21, 2000-04-…\n$ wk1          &lt;dbl&gt; 87, 91, 81, 76, 57, 51, 97, 84, 59, 76, 84, 57, 50, 71, 7…\n$ wk2          &lt;dbl&gt; 82, 87, 70, 76, 34, 39, 97, 62, 53, 76, 84, 47, 39, 51, 6…\n$ wk3          &lt;dbl&gt; 72, 92, 68, 72, 25, 34, 96, 51, 38, 74, 75, 45, 30, 28, 5…\n$ wk4          &lt;dbl&gt; 77, NA, 67, 69, 17, 26, 95, 41, 28, 69, 73, 29, 28, 18, 4…\n$ wk5          &lt;dbl&gt; 87, NA, 66, 67, 17, 26, 100, 38, 21, 68, 73, 23, 21, 13, …\n$ wk6          &lt;dbl&gt; 94, NA, 57, 65, 31, 19, NA, 35, 18, 67, 69, 18, 19, 13, 3…\n$ wk7          &lt;dbl&gt; 99, NA, 54, 55, 36, 2, NA, 35, 16, 61, 68, 11, 20, 11, 34…\n$ wk8          &lt;dbl&gt; NA, NA, 53, 59, 49, 2, NA, 38, 14, 58, 65, 9, 17, 1, 29, …\n$ wk9          &lt;dbl&gt; NA, NA, 51, 62, 53, 3, NA, 38, 12, 57, 73, 9, 17, 1, 27, …\n$ wk10         &lt;dbl&gt; NA, NA, 51, 61, 57, 6, NA, 36, 10, 59, 83, 11, 17, 2, 30,…\n$ wk11         &lt;dbl&gt; NA, NA, 51, 61, 64, 7, NA, 37, 9, 66, 92, 1, 17, 2, 36, N…\n$ wk12         &lt;dbl&gt; NA, NA, 51, 59, 70, 22, NA, 37, 8, 68, NA, 1, 3, 3, 37, N…\n$ wk13         &lt;dbl&gt; NA, NA, 47, 61, 75, 29, NA, 38, 6, 61, NA, 1, 3, 3, 39, N…\n$ wk14         &lt;dbl&gt; NA, NA, 44, 66, 76, 36, NA, 49, 1, 67, NA, 1, 7, 4, 49, N…\n$ wk15         &lt;dbl&gt; NA, NA, 38, 72, 78, 47, NA, 61, 2, 59, NA, 4, 10, 12, 57,…\n$ wk16         &lt;dbl&gt; NA, NA, 28, 76, 85, 67, NA, 63, 2, 63, NA, 8, 17, 11, 63,…\n$ wk17         &lt;dbl&gt; NA, NA, 22, 75, 92, 66, NA, 62, 2, 67, NA, 12, 25, 13, 65…\n$ wk18         &lt;dbl&gt; NA, NA, 18, 67, 96, 84, NA, 67, 2, 71, NA, 22, 29, 15, 68…\n$ wk19         &lt;dbl&gt; NA, NA, 18, 73, NA, 93, NA, 83, 3, 79, NA, 23, 29, 18, 79…\n$ wk20         &lt;dbl&gt; NA, NA, 14, 70, NA, 94, NA, 86, 4, 89, NA, 43, 40, 20, 86…\n$ wk21         &lt;dbl&gt; NA, NA, 12, NA, NA, NA, NA, NA, 5, NA, NA, 44, 43, 30, NA…\n$ wk22         &lt;dbl&gt; NA, NA, 7, NA, NA, NA, NA, NA, 5, NA, NA, NA, 50, 40, NA,…\n$ wk23         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 6, NA, NA, NA, NA, 39, NA,…\n$ wk24         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 9, NA, NA, NA, NA, 44, NA,…\n$ wk25         &lt;dbl&gt; NA, NA, 6, NA, NA, NA, NA, NA, 13, NA, NA, NA, NA, NA, NA…\n$ wk26         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, 14, NA, NA, NA, NA, NA, NA…\n$ wk27         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, 16, NA, NA, NA, NA, NA, NA…\n$ wk28         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 23, NA, NA, NA, NA, NA, NA…\n$ wk29         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 22, NA, NA, NA, NA, NA, NA…\n$ wk30         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 33, NA, NA, NA, NA, NA, NA…\n$ wk31         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, 36, NA, NA, NA, NA, NA, NA…\n$ wk32         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, 43, NA, NA, NA, NA, NA, NA…\n$ wk33         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk34         &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk35         &lt;dbl&gt; NA, NA, 4, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk36         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk37         &lt;dbl&gt; NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk38         &lt;dbl&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk39         &lt;dbl&gt; NA, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ wk40         &lt;dbl&gt; NA, NA, 15, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk41         &lt;dbl&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk42         &lt;dbl&gt; NA, NA, 13, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk43         &lt;dbl&gt; NA, NA, 14, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk44         &lt;dbl&gt; NA, NA, 16, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk45         &lt;dbl&gt; NA, NA, 17, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk46         &lt;dbl&gt; NA, NA, 21, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk47         &lt;dbl&gt; NA, NA, 22, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk48         &lt;dbl&gt; NA, NA, 24, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk49         &lt;dbl&gt; NA, NA, 28, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk50         &lt;dbl&gt; NA, NA, 33, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk51         &lt;dbl&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk52         &lt;dbl&gt; NA, NA, 42, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk53         &lt;dbl&gt; NA, NA, 49, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk54         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk55         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk56         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk57         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk58         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk59         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk60         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk61         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk62         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk63         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk64         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk65         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk66         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk67         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk68         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk69         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk70         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk71         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk72         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk73         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk74         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk75         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wk76         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nHere, you can immediately see the problem: it contains two types of observations: songs and ranks. Hence, the data set needs to be split up. However, there should be a pointer from the rank data set to the song data set. First, I add an ID column to song_tbl. Then, I can add it to rank_tbl and drop the unnecessary columns which contain the name of the artist and the track.\n\nsong_tbl &lt;- billboard |&gt; \n  rowid_to_column(\"song_id\") |&gt; \n  distinct(artist, track, .keep_all = TRUE) |&gt; \n  select(song_id:track)\n\nglimpse(song_tbl)\n\nRows: 317\nColumns: 3\n$ song_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ artist  &lt;chr&gt; \"2 Pac\", \"2Ge+her\", \"3 Doors Down\", \"3 Doors Down\", \"504 Boyz\"…\n$ track   &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"The Hardest Part Of ...\", \"Krypton…\n\n\n\nrank_tbl &lt;- billboard |&gt; \n  pivot_longer(cols = starts_with(\"wk\"), \n               names_to = \"week\", \n               names_prefix = \"wk\", \n               values_to = \"rank\") |&gt; \n  mutate(week = as.numeric(week),\n         date = date.entered + (week-1) * 7) |&gt; \n  drop_na() |&gt; \n  left_join(song_tbl, by = c(\"artist\", \"track\")) |&gt; \n  select(song_id, date, week, rank)\n\nglimpse(rank_tbl)\n\nRows: 5,307\nColumns: 4\n$ song_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ date    &lt;date&gt; 2000-02-26, 2000-03-04, 2000-03-11, 2000-03-18, 2000-03-25, 2…\n$ week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ rank    &lt;dbl&gt; 87, 82, 72, 77, 87, 94, 99, 91, 87, 92, 81, 70, 68, 67, 66, 57…\n\n\n\n\nOne unit is in multiple tables\nFor this example, I have split up a data set from the socviz package containing data on the 2016 elections in the U.S. according to census region and stored them in a folder. I can scrape the file names in the folder and read it into a list in an automated manner. (Note that the functions used to read the files in in an automated fashion are beyond the scope of this course. They come from the fs (Hester, Wickham, and Csárdi 2021) and the purrr package (Henry and Wickham 2020).)3\n\nfile_list &lt;- dir_ls(path = \"data/socviz_us\") |&gt; \n  map(read_csv,\n      col_types = cols(\n        id = col_double(),\n        name = col_character(),\n        state = col_character(),\n        census_region = col_character(),\n        pop_dens = col_character(),\n        pop_dens4 = col_character(),\n        pop_dens6 = col_character(),\n        pct_black = col_character(),\n        pop = col_double(),\n        female = col_double(),\n        white = col_double(),\n        black = col_double(),\n        travel_time = col_double(),\n        land_area = col_double(),\n        hh_income = col_double(),\n        su_gun4 = col_character(),\n        su_gun6 = col_character(),\n        fips = col_double(),\n        votes_dem_2016 = col_double(),\n        votes_gop_2016 = col_double(),\n        total_votes_2016 = col_double(),\n        per_dem_2016 = col_double(),\n        per_gop_2016 = col_double(),\n        diff_2016 = col_double(),\n        per_dem_2012 = col_double(),\n        per_gop_2012 = col_double(),\n        diff_2012 = col_double(),\n        winner = col_character(),\n        partywinner16 = col_character(),\n        winner12 = col_character(),\n        partywinner12 = col_character(),\n        flipped = col_character()\n))\n\nThe list now consists of four tibbles in a list which need to be bound together. You can achieve this using list_rbind(). Its counterpart is list_cbind() which binds columns together. It matches rows by position.\n\nelection_data &lt;- file_list |&gt; list_rbind()\nglimpse(election_data)\n\nRows: 3,141\nColumns: 32\n$ id               &lt;dbl&gt; 17001, 17003, 17005, 17007, 17009, 17011, 17013, 1701…\n$ name             &lt;chr&gt; \"Adams County\", \"Alexander County\", \"Bond County\", \"B…\n$ state            &lt;chr&gt; \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\", \"IL\",…\n$ census_region    &lt;chr&gt; \"Midwest\", \"Midwest\", \"Midwest\", \"Midwest\", \"Midwest\"…\n$ pop_dens         &lt;chr&gt; \"[   50,  100)\", \"[   10,   50)\", \"[   10,   50)\", \"[…\n$ pop_dens4        &lt;chr&gt; \"[ 45,  118)\", \"[ 17,   45)\", \"[ 45,  118)\", \"[118,71…\n$ pop_dens6        &lt;chr&gt; \"[ 45,   82)\", \"[ 25,   45)\", \"[ 45,   82)\", \"[ 82,  …\n$ pct_black        &lt;chr&gt; \"[ 2.0, 5.0)\", \"[25.0,50.0)\", \"[ 5.0,10.0)\", \"[ 2.0, …\n$ pop              &lt;dbl&gt; 66988, 7492, 17269, 53869, 6832, 33840, 4956, 14715, …\n$ female           &lt;dbl&gt; 51.3, 49.5, 47.5, 50.2, 35.5, 51.0, 49.7, 50.1, 49.1,…\n$ white            &lt;dbl&gt; 93.7, 60.6, 90.9, 93.2, 78.6, 96.8, 98.8, 96.7, 93.2,…\n$ black            &lt;dbl&gt; 3.7, 36.1, 6.5, 2.6, 19.1, 0.8, 0.3, 1.1, 4.4, 12.8, …\n$ travel_time      &lt;dbl&gt; 16.6, 25.6, 23.6, 30.1, 18.9, 20.4, 39.6, 23.8, 22.2,…\n$ land_area        &lt;dbl&gt; 855.20, 235.51, 380.28, 280.72, 305.61, 869.03, 253.8…\n$ hh_income        &lt;dbl&gt; 45073, 26972, 48163, 60893, 42194, 48977, 50436, 4798…\n$ su_gun4          &lt;chr&gt; \"[ 0, 5)\", \"[ 5, 8)\", \"[ 0, 5)\", \"[ 0, 5)\", \"[ 0, 5)\"…\n$ su_gun6          &lt;chr&gt; \"[ 4, 7)\", \"[ 7, 8)\", \"[ 4, 7)\", \"[ 0, 4)\", \"[ 0, 4)\"…\n$ fips             &lt;dbl&gt; 17001, 17003, 17005, 17007, 17009, 17011, 17013, 1701…\n$ votes_dem_2016   &lt;dbl&gt; 7633, 1262, 2066, 8952, 475, 6010, 739, 2437, 1617, 4…\n$ votes_gop_2016   &lt;dbl&gt; 22732, 1496, 4884, 12261, 1776, 9264, 1719, 4428, 321…\n$ total_votes_2016 &lt;dbl&gt; 31770, 2820, 7462, 22604, 2336, 16303, 2556, 7354, 50…\n$ per_dem_2016     &lt;dbl&gt; 0.2402581, 0.4475177, 0.2768695, 0.3960361, 0.2033390…\n$ per_gop_2016     &lt;dbl&gt; 0.7155178, 0.5304965, 0.6545162, 0.5424261, 0.7602740…\n$ diff_2016        &lt;dbl&gt; 15099, 234, 2818, 3309, 1301, 3254, 980, 1991, 1599, …\n$ per_dem_2012     &lt;dbl&gt; 0.3152466, 0.5610873, 0.4122471, 0.4625697, 0.3331922…\n$ per_gop_2012     &lt;dbl&gt; 0.6670705, 0.4248927, 0.5591853, 0.5195706, 0.6397121…\n$ diff_2012        &lt;dbl&gt; 10744, 476, 1075, 1216, 724, 33, 360, 107, 657, 5292,…\n$ winner           &lt;chr&gt; \"Trump\", \"Trump\", \"Trump\", \"Trump\", \"Trump\", \"Trump\",…\n$ partywinner16    &lt;chr&gt; \"Republican\", \"Republican\", \"Republican\", \"Republican…\n$ winner12         &lt;chr&gt; \"Romney\", \"Obama\", \"Romney\", \"Romney\", \"Romney\", \"Rom…\n$ partywinner12    &lt;chr&gt; \"Republican\", \"Democrat\", \"Republican\", \"Republican\",…\n$ flipped          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n\n\nHowever, the topic of this script is different joins. The dplyr package offers six different joins: left_join(), right_join(), inner_join(), full_join(), semi_join(), and anti_join(). The former four are mutating joins, they add columns. The latter two can be used to filter rows in a data set. Below is an overview from the dplyr cheat sheet:\n\n\n\nOverview of the different joins\n\n\nIn the following, I will illustrate this using the election data. I split up the data set into three: data on the elections 2016 and 2012, and demographic data. The column they have in common is the county’s respective name.\n\nelection_data16 &lt;- election_data |&gt; \n  select(name, state, votes_dem_2016:diff_2016, winner, partywinner16)\n\nelection_data12 &lt;- election_data |&gt; \n  select(name, state, per_dem_2012:partywinner12)\n\ndemographic_data &lt;- election_data |&gt; \n  select(name, state, pop:hh_income) |&gt; \n  slice(1:2000) #you will see later why I do this\n\n\n\nleft_join() and right_join()\nIf we want to add the demographic data to the election data 2016, we can use a left_join() or a right_join(). The former adds all columns of y to x, the latter all columns of x to y. Here, I want to add the demographic data to the election data 2016. Therefore, I use a left_join():\n\nelection_data16 |&gt; \n  left_join(demographic_data)\n\nJoining with `by = join_by(name, state)`\n\n\n# A tibble: 3,141 × 17\n   name        state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adams Coun… IL              7633          22732            31770        0.240\n 2 Alexander … IL              1262           1496             2820        0.448\n 3 Bond County IL              2066           4884             7462        0.277\n 4 Boone Coun… IL              8952          12261            22604        0.396\n 5 Brown Coun… IL               475           1776             2336        0.203\n 6 Bureau Cou… IL              6010           9264            16303        0.369\n 7 Calhoun Co… IL               739           1719             2556        0.289\n 8 Carroll Co… IL              2437           4428             7354        0.331\n 9 Cass County IL              1617           3216             5054        0.320\n10 Champaign … IL             49694          33235            89196        0.557\n# ℹ 3,131 more rows\n# ℹ 11 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;, pop &lt;dbl&gt;, female &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;,\n#   travel_time &lt;dbl&gt;, land_area &lt;dbl&gt;, hh_income &lt;dbl&gt;\n\n\nIf the column that both data sets have in common has the same name, there’s no need to provide it. If this is not the case, you need to provide it in a character vector:\n\nelection_data16 |&gt; \n  rename(county = name) |&gt; \n  right_join(demographic_data, by = join_by(\"county\" == \"name\"))\n\nWarning in right_join(rename(election_data16, county = name), demographic_data, : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 10,348 × 18\n   county    state.x votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adams Co… IL                7633          22732            31770        0.240\n 2 Adams Co… IL                7633          22732            31770        0.240\n 3 Adams Co… IL                7633          22732            31770        0.240\n 4 Adams Co… IL                7633          22732            31770        0.240\n 5 Adams Co… IL                7633          22732            31770        0.240\n 6 Adams Co… IL                7633          22732            31770        0.240\n 7 Adams Co… IL                7633          22732            31770        0.240\n 8 Adams Co… IL                7633          22732            31770        0.240\n 9 Adams Co… IL                7633          22732            31770        0.240\n10 Alexande… IL                1262           1496             2820        0.448\n# ℹ 10,338 more rows\n# ℹ 12 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;, state.y &lt;chr&gt;, pop &lt;dbl&gt;, female &lt;dbl&gt;, white &lt;dbl&gt;,\n#   black &lt;dbl&gt;, travel_time &lt;dbl&gt;, land_area &lt;dbl&gt;, hh_income &lt;dbl&gt;\n\n\nHere, the problem is that the same counties exist in different states. Therefore, all combinations are returned. Hence, I need to specify two arguments: the county’s name and state.\n\nelection_data16 |&gt; \n  rename(county = name) |&gt; \n  right_join(demographic_data, by = join_by(\"county\" == \"name\", \"state\"))\n\n# A tibble: 2,000 × 17\n   county      state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adams Coun… IL              7633          22732            31770        0.240\n 2 Alexander … IL              1262           1496             2820        0.448\n 3 Bond County IL              2066           4884             7462        0.277\n 4 Boone Coun… IL              8952          12261            22604        0.396\n 5 Brown Coun… IL               475           1776             2336        0.203\n 6 Bureau Cou… IL              6010           9264            16303        0.369\n 7 Calhoun Co… IL               739           1719             2556        0.289\n 8 Carroll Co… IL              2437           4428             7354        0.331\n 9 Cass County IL              1617           3216             5054        0.320\n10 Champaign … IL             49694          33235            89196        0.557\n# ℹ 1,990 more rows\n# ℹ 11 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;, pop &lt;dbl&gt;, female &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;,\n#   travel_time &lt;dbl&gt;, land_area &lt;dbl&gt;, hh_income &lt;dbl&gt;\n\n\nLeft joins return all rows which are in x. If a column is in x but not in y, an NA will be included at this position. Right joins work vice versa and return all rows which are in y.\n\n\ninner_join()\nAn inner_join() returns all rows which are in x and y.\n\nelection_data16 |&gt; \n  inner_join(demographic_data)\n\nJoining with `by = join_by(name, state)`\n\n\n# A tibble: 2,000 × 17\n   name        state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adams Coun… IL              7633          22732            31770        0.240\n 2 Alexander … IL              1262           1496             2820        0.448\n 3 Bond County IL              2066           4884             7462        0.277\n 4 Boone Coun… IL              8952          12261            22604        0.396\n 5 Brown Coun… IL               475           1776             2336        0.203\n 6 Bureau Cou… IL              6010           9264            16303        0.369\n 7 Calhoun Co… IL               739           1719             2556        0.289\n 8 Carroll Co… IL              2437           4428             7354        0.331\n 9 Cass County IL              1617           3216             5054        0.320\n10 Champaign … IL             49694          33235            89196        0.557\n# ℹ 1,990 more rows\n# ℹ 11 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;, pop &lt;dbl&gt;, female &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;,\n#   travel_time &lt;dbl&gt;, land_area &lt;dbl&gt;, hh_income &lt;dbl&gt;\n\n\n\n\nfull_join()\nA full_join() returns rows and columns from both x and y.\n\nelection_data16 |&gt; \n  full_join(demographic_data)\n\nJoining with `by = join_by(name, state)`\n\n\n# A tibble: 3,141 × 17\n   name        state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adams Coun… IL              7633          22732            31770        0.240\n 2 Alexander … IL              1262           1496             2820        0.448\n 3 Bond County IL              2066           4884             7462        0.277\n 4 Boone Coun… IL              8952          12261            22604        0.396\n 5 Brown Coun… IL               475           1776             2336        0.203\n 6 Bureau Cou… IL              6010           9264            16303        0.369\n 7 Calhoun Co… IL               739           1719             2556        0.289\n 8 Carroll Co… IL              2437           4428             7354        0.331\n 9 Cass County IL              1617           3216             5054        0.320\n10 Champaign … IL             49694          33235            89196        0.557\n# ℹ 3,131 more rows\n# ℹ 11 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;, pop &lt;dbl&gt;, female &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;,\n#   travel_time &lt;dbl&gt;, land_area &lt;dbl&gt;, hh_income &lt;dbl&gt;\n\n\n\n\nsemi_join()\nFiltering joins only keep the cases from x, no data set is added.\nThe semi_join() returns all rows from x with matching values in y. You can compare it to a right_join() but without adding the columns of y.\n\nelection_data16 |&gt; \n  semi_join(demographic_data)\n\nJoining with `by = join_by(name, state)`\n\n\n# A tibble: 2,000 × 10\n   name        state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Adams Coun… IL              7633          22732            31770        0.240\n 2 Alexander … IL              1262           1496             2820        0.448\n 3 Bond County IL              2066           4884             7462        0.277\n 4 Boone Coun… IL              8952          12261            22604        0.396\n 5 Brown Coun… IL               475           1776             2336        0.203\n 6 Bureau Cou… IL              6010           9264            16303        0.369\n 7 Calhoun Co… IL               739           1719             2556        0.289\n 8 Carroll Co… IL              2437           4428             7354        0.331\n 9 Cass County IL              1617           3216             5054        0.320\n10 Champaign … IL             49694          33235            89196        0.557\n# ℹ 1,990 more rows\n# ℹ 4 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;\n\n\n\n\nanti_join()\nanti_join() returns all rows from x with no matching rows in y.\n\nelection_data16 |&gt; \n  anti_join(demographic_data)\n\nJoining with `by = join_by(name, state)`\n\n\n# A tibble: 1,141 × 10\n   name        state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 Onslow Cou… NC             17156          36342            55364        0.310\n 2 Orange Cou… NC             59105          18373            79830        0.740\n 3 Pamlico Co… NC              2427           4225             6772        0.358\n 4 Pasquotank… NC              8455           8082            16964        0.498\n 5 Pender Cou… NC              9086          17317            27072        0.336\n 6 Perquimans… NC              2291           4143             6595        0.347\n 7 Person Cou… NC              7772          11116            19303        0.403\n 8 Pitt County NC             40967          35191            78264        0.523\n 9 Polk County NC              3715           6738            10723        0.346\n10 Randolph C… NC             13074          49156            63615        0.206\n# ℹ 1,131 more rows\n# ℹ 4 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;\n\n\n\n\n\nbind_rows() and bind_cols()\nBinding tibbles together is made easy using the bind_*() functions. bind_rows() binds them together by rows, bind_cols() by columns. For the former, it is important that column names are matching. Otherwise, the non-matching ones will be added as separate columns and NAs introduced. IDs can be added by using the .id = argument, where the name of the id column can be specified.\n\nelection_data16 |&gt; \n  semi_join(demographic_data) |&gt; \n  bind_rows(election_data16 |&gt;\n              anti_join(demographic_data),\n            .id = \"id\")\n\nJoining with `by = join_by(name, state)`\nJoining with `by = join_by(name, state)`\n\n\n# A tibble: 3,141 × 11\n   id    name  state votes_dem_2016 votes_gop_2016 total_votes_2016 per_dem_2016\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1 1     Adam… IL              7633          22732            31770        0.240\n 2 1     Alex… IL              1262           1496             2820        0.448\n 3 1     Bond… IL              2066           4884             7462        0.277\n 4 1     Boon… IL              8952          12261            22604        0.396\n 5 1     Brow… IL               475           1776             2336        0.203\n 6 1     Bure… IL              6010           9264            16303        0.369\n 7 1     Calh… IL               739           1719             2556        0.289\n 8 1     Carr… IL              2437           4428             7354        0.331\n 9 1     Cass… IL              1617           3216             5054        0.320\n10 1     Cham… IL             49694          33235            89196        0.557\n# ℹ 3,131 more rows\n# ℹ 4 more variables: per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner &lt;chr&gt;,\n#   partywinner16 &lt;chr&gt;\n\n\nFor bind_cols(), the length has to be the same. Duplicated column names will be changed.\n\nelection_data12 |&gt; bind_cols(election_data16)\n\nNew names:\n• `name` -&gt; `name...1`\n• `state` -&gt; `state...2`\n• `winner` -&gt; `winner...6`\n• `partywinner16` -&gt; `partywinner16...7`\n• `name` -&gt; `name...10`\n• `state` -&gt; `state...11`\n• `winner` -&gt; `winner...18`\n• `partywinner16` -&gt; `partywinner16...19`\n\n\n# A tibble: 3,141 × 19\n   name...1         state...2 per_dem_2012 per_gop_2012 diff_2012 winner...6\n   &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n 1 Adams County     IL               0.315        0.667     10744 Trump     \n 2 Alexander County IL               0.561        0.425       476 Trump     \n 3 Bond County      IL               0.412        0.559      1075 Trump     \n 4 Boone County     IL               0.463        0.520      1216 Trump     \n 5 Brown County     IL               0.333        0.640       724 Trump     \n 6 Bureau County    IL               0.489        0.491        33 Trump     \n 7 Calhoun County   IL               0.419        0.559       360 Trump     \n 8 Carroll County   IL               0.496        0.482       107 Trump     \n 9 Cass County      IL               0.422        0.557       657 Trump     \n10 Champaign County IL               0.520        0.452      5292 Clinton   \n# ℹ 3,131 more rows\n# ℹ 13 more variables: partywinner16...7 &lt;chr&gt;, winner12 &lt;chr&gt;,\n#   partywinner12 &lt;chr&gt;, name...10 &lt;chr&gt;, state...11 &lt;chr&gt;,\n#   votes_dem_2016 &lt;dbl&gt;, votes_gop_2016 &lt;dbl&gt;, total_votes_2016 &lt;dbl&gt;,\n#   per_dem_2016 &lt;dbl&gt;, per_gop_2016 &lt;dbl&gt;, diff_2016 &lt;dbl&gt;, winner...18 &lt;chr&gt;,\n#   partywinner16...19 &lt;chr&gt;\n\n\n\n\nFurther links\n\nChapter in R4DS\nMore on window functions in the vignette: vignette(\"window-functions\")\nAgain, the cheatsheet\nA tutorial on YouTube\nAnother introduction can be found here.\nThe chapter in R4DS has some nice diagrams.\nYou can also consult the introverse package if you need help with the packages covered here – introverse::show_topics(\"dplyr\") will give you an overview of dplyr’s functions, and get_help(\"name of function\") will help you with the respective function.\n\n\n\nExercises\nOpen the IMDb file (click to download).\n\nFind the duplicated movie. How could you go across this?\nWhich director has made the longest movie?\nWhat’s the highest ranked movie?\nWhich movie got the most votes?\nWhich movie had the biggest revenue in 2016?\nHow much revenue did the movies in the data set make each year in total?\nFilter movies following some conditions:\n\nMore runtime than the average runtime (hint: you could also use mutate() before).\nMovies directed by J. J. Abrams.\nMore votes than the median of all of the votes.\nThe movies which have the most common value (the mode) in terms of rating (mode() does exist but will not work in the way you might like it to work – run the script below and use the my_mode function).\n\n\n\n## helper function for mode\n\nmy_mode &lt;- function(x){ \n    ta = table(x)\n    tam = max(ta)\n    if (all(ta == tam))\n         mod = NA\n    else\n         if(is.numeric(x))\n    mod = as.numeric(names(ta)[ta == tam])\n    else\n         mod = names(ta)[ta == tam]\n    return(mod)\n}"
  },
  {
    "objectID": "2_r_catch-up.html#visualization",
    "href": "2_r_catch-up.html#visualization",
    "title": "Chapter 2: Brief R Recap",
    "section": "Visualizations with ggplot2",
    "text": "Visualizations with ggplot2\n\n“The purpose of visualization is insight, not pictures.” – Ben A. Shneiderman\n\nIn R, the dominant package for visualizing data is ggplot2 which belongs to the tidyverse.\n\nThe “layered grammar of graphics”\nggplot2 works with tibbles and the data needs to be in a tidy format. It builds graphics using “the layered grammar of graphics.” (Wickham 2010)\n\npublishers &lt;- read_csv(\"data/publishers_with_places.csv\")\n  \npublishers_filtered &lt;- publishers |&gt; \n  group_by(city) |&gt; \n  filter(n() &gt; 5) |&gt; \n  drop_na()\n\nThis implies that you start with a base layer – the initial ggplot2 call.\n\npublishers_filtered |&gt; \nggplot()\n\n\n\n\n\n\n\n\nThe initial call produces an empty coordinate system. It can be filled with additional layers.\n\nggplot(data = publishers_filtered) +\n  geom_bar(aes(x = city)) \n\n\n\n\n\n\n\n\nUnlike the remainder of the tidyverse, ggplot2 uses a + instead of the pipe |&gt;. If you use the pipe by accident, it will not work and an (informative) error message will appear.\n\n# ggplot(data = publishers_filtered) |&gt; \n#   geom_bar(aes(x = city)) \n\n\n\nThe layers\nIn general, a call looks like this:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nAs you might have seen above, I provided the data in the initial ggplot call. Then, when I added the layer – the geom_bar() for a bar plot – I had to provide the mapping – which variables I wanted to plot – using aes(). This is referred to as the aesthetics. In my case, I wanted the cities to be projected to the x-axis. Since I was using geom_bar to create a bar plot, the number of occurrences of the respective cities were automatically counted and depicted on the y-axis. There are more geom_* functions and they all create different plots. Whether you can use them or not depends on the data you have at hand and/or the number of variables you want to plot. In the following, I will give you a brief overview of the most important geoms.\n\nOne variable\nIf you only want to display one variable, the x- or y-axis, as you choose, will depict the variable’s value. The counterpart will display the frequency or density of those values.\n\nOne variable – discrete\nHere, the only possible kind of visualization is a bar plot as shown above. If the visualization should look more fancy, e.g., with colored bars, you have several arguments at hand. If they should not be different for different kinds of data, they need to be specified outside the aes(). There are always different arguments and you can look them up using ?&lt;GEOM_FUNCTION&gt; and then looking at the Aesthetics section. Apart from that, you can also look at the ggplot2 cheatsheet.\n\nggplot(data = publishers_filtered) +\n  geom_bar(aes(x = city), fill = \"blue\") \n\n\n\n\n\n\n\n\n\n\nOne variable – continuous\nIf you want to display a continuous variable’s distribution of values, you can use a histogram. Its geom_* function is geom_histogram():\n\nbillboard &lt;- read_csv(\"data/billboard.csv\")\n\nRows: 317 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (2): artist, track\ndbl  (65): wk1, wk2, wk3, wk4, wk5, wk6, wk7, wk8, wk9, wk10, wk11, wk12, wk...\nlgl  (11): wk66, wk67, wk68, wk69, wk70, wk71, wk72, wk73, wk74, wk75, wk76\ndate  (1): date.entered\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsong_tbl &lt;- billboard |&gt; \n  distinct(artist, track) |&gt; \n  mutate(song_id = row_number())\n\nrank_tbl &lt;- billboard |&gt; \n  pivot_longer(cols = starts_with(\"wk\"), \n               names_to = \"week\", \n               names_prefix = \"wk\", \n               values_to = \"rank\") |&gt; \n  mutate(week = as.numeric(week),\n         date = date.entered + (week-1) * 7) |&gt; \n  drop_na() |&gt; \n  left_join(song_tbl, by = c(\"artist\", \"track\")) |&gt; \n  select(song_id, date, week, rank)\n\nHow does the distribution of songs over the weeks look like?\n\nggplot(data = rank_tbl) +\n  geom_histogram(aes(x = week))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nA smoothed histogram is geom_density():\n\nggplot(data = rank_tbl) +\n  geom_density(aes(x = week))\n\n\n\n\n\n\n\n\n\n\n\nTwo variables\nIn the majority of cases, you will want to display the relationship between two variables, one on the x- and the other one on the y-axis.\n\nBoth continuous\n\ncounty_data_midwest &lt;- socviz::county_data |&gt; \n  filter(census_region == \"Midwest\") |&gt; \n  drop_na()\n\nIf both variables are continuous, the easiest option is to use a scatter plot.\n\nggplot(data = county_data_midwest) +\n  geom_point(aes(x = per_dem_2016, y = per_gop_2016))\n\n\n\n\n\n\n\n\nIf you don’t like dots, the shape = argument allows you to change the shape of the data points. There are also other arguments to change, for instance, transparency (alpha =) or size (size =). Find an overview of the allowed aesthetic specifications here.\n\nggplot(data = county_data_midwest) +\n  geom_point(aes(x = per_dem_2016, y = per_gop_2016), \n             shape = \"cross\", \n             size = 2)\n\n\n\n\n\n\n\n\nHere, it might make sense to color the points according to a categorical variable (state, in this case). If so, a legend is added which maps the colors to their respective values.\n\nggplot(data = county_data_midwest) +\n  geom_point(aes(x = per_dem_2016, y = per_gop_2016)) \n\n\n\n\n\n\n\n\nSince I look at the relationship between votes for the Republicans and the Democrats, and the U.S. is a two-party system, there is a fairly clear relationship between them both. This can also be depicted using geom_smooth():\n\nggplot(data = county_data_midwest) +\n  geom_smooth(aes(x = per_dem_2016, y = per_gop_2016, color = state))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere, color = state has a different effect: each dimension of the categorical variable gets its own line.\nIf you do not want it to be smoothed, just use geom_line().\n\nggplot(data = county_data_midwest) +\n  geom_line(aes(x = per_dem_2016, y = per_gop_2016), color = \"grey\") \n\n\n\n\n\n\n\n\n\n\nDiscrete X, continuous Y\nIn this case, different categories of data will be put on the x-axis and some of their properties will be displayed on the y-axis. The probably most prominent example for this type of plot is a box plot:\n\nggplot(data = county_data_midwest) +\n  geom_boxplot(aes(x = state, y = per_gop_2016))\n\n\n\n\n\n\n\n\n\n\nBoth discrete\nIt is rarely the case that you want to depict two categorical variables in one plot. If so, you can use geom_jitter(). It is related to geom_point(). The difference is that with geom_jitter(), a little bit of noise is added to the dots, making them appear distinct.\n\nggplot(data = county_data_midwest) +\n  geom_jitter(aes(x = state, y = winner))\n\n\n\n\n\n\n\n\nAs opposed to:\n\nggplot(data = county_data_midwest) +\n  geom_point(aes(x = state, y = winner))\n\n\n\n\n\n\n\n\n\n\n\n\nMaking graphs “publishable”\nSo far, I have only added one layer to the plot. This suffices for the most basic visualizations. The good thing about R and RMarkdown is, however, that you can write entire publications only using their means. Hence, the plots need to look awesome. This section is dedicated to how you can achieve this. First, I will touch upon how you can make them look good using scales. labs() allow you to add titles, captions, and axis labels. Finally, facet_* allows you to plot multiple plots into one.\n\nScales\nScales can be used to take control of how the data’s values are mapped to the aesthetic’s visual values. You can find a more exhaustive tutorial on them here.\n\nscale_*_continuous – for dealing with continuous values. (you can find an exhaustive list of colors in R here)\n\n\nggplot(data = county_data_midwest) +\n  geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = white)) +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  scale_color_gradient(low = \"green\",\n                       high = \"red\")\n\n\n\n\n\n\n\n\n\nscale_*_discrete – for dealing with discrete values\nscale_*_manual – manually mapping discrete values to visual values\n\n\nsocviz::county_data |&gt; \n  filter(state %in% c(\"IA\", \"IL\", \"IN\", \"KS\")) |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = state)) +\n    scale_color_manual(values = c(\"IA\" = \"blue\", \n                                  \"IL\" = \"green\", \n                                  \"IN\" = \"red\", \n                                  \"KS\" = \"purple\"),\n                       name = \"State\",\n                       labels = c(\"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\")) \n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nAdding titles, captions, etc.\nNow you have modified the scales and colors – there is a lot more to be modified if you want to – but you have not added a meaningful title, a nice caption (where were the data obtained?), and the axes do not have proper names, too. This can be achieved using labs() (which is the abbreviation for labels).\n\nsocviz::county_data |&gt; \n  filter(state %in% c(\"IA\", \"IL\", \"IN\", \"KS\")) |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = state)) +\n    scale_color_manual(values = c(\"IA\" = \"blue\", \n                                  \"IL\" = \"green\", \n                                  \"IN\" = \"red\", \n                                  \"KS\" = \"purple\"),\n                       name = \"State\",\n                       breaks = waiver(),\n                       labels = c(\"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\")) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    ggtitle(\"Relationship between percentages of votes for Democrats and Republicans in selected states in the Midwest\") +\n    xlab(\"Percentage of votes for the Democrats in 2016\") +\n    ylab(\"Percentage of votes for the Republicans in 2016\") \n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWell, that doesn’t look good, the title is too long. Inserting \\n – for new line – will do the trick.\n\nsocviz::county_data |&gt; \n  filter(state %in% c(\"IA\", \"IL\", \"IN\", \"KS\")) |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = state)) +\n    scale_color_manual(values = c(\"IA\" = \"blue\", \"IL\" = \"green\", \"IN\" = \"red\", \"KS\" = \"purple\"),\n                      name = \"State\",\n                      breaks = waiver(),\n                      labels = c(\"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\")) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    ggtitle(\"Relationship between percentages of votes for Democrats \\nand Republicans in selected states in the Midwest\") +\n    xlab(\"Percentage of votes for the Democrats in 2016\") +\n    ylab(\"Percentage of votes for the Republicans in 2016\") \n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHowever, providing it with three different layers just for labeling is pretty tedious. This is where labs() comes in handy.\n\nsocviz::county_data |&gt; \n  filter(state %in% c(\"IA\", \"IL\", \"IN\", \"KS\")) |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = state)) +\n    scale_color_manual(values = c(\"IA\" = \"blue\", \"IL\" = \"green\", \"IN\" = \"red\", \"KS\" = \"purple\"),\n                      name = \"State\",\n                      breaks = waiver(),\n                      labels = c(\"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\")) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    labs(title = \"Relationship between percentages of votes for Democrats \\nand Republicans in selected states in the Midwest\",\n         caption = \"Data obtained from the socviz R package\",\n         x = \"Percentage of votes for the Democrats in 2016\",\n         y = \"Percentage of votes for the Republicans in 2016\") \n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nFacets\nThe original data set consists of four different census regions. If I were to compare them, I could color them accordingly.\n\nsocviz::county_data |&gt; \n  drop_na() |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = census_region)) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    scale_color_discrete()\n\n\n\n\n\n\n\n\nDespite the coloring according to the different states, it is still hard to assess whether there really are differences. Apart from that, I would like to assess the impact the percentage of white people in the population has. This would be easier if I put them into individual graphs. I can achieve this using so-called facets. Facets enable me to divide the plot into subplots based on categorical variables. facet_wrap() puts them into a rectangular layout. The categorical variable needs to be provided prefixed with a tilde ~, nrow determines the number of rows.\n\nsocviz::county_data |&gt; \n  drop_na() |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = white)) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    scale_color_gradient(low = \"green\",\n                         high = \"red\") +\n    facet_wrap(vars(census_region),\n               nrow = 2)\n\n\n\n\n\n\n\n\nApart from that, I can also spread it out using two different variables. Here, I will look at differences in the distribution of whites in the counties split up by who won in 2016 and 2012. This can be achieved using facet_grid(categorical_variable_1~categorical_variable_2). The former one will be put into rows, the latter into columns.\n\nsocviz::county_data |&gt; \n  drop_na() |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = white)) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    scale_color_gradient(low = \"green\",\n                         high = \"red\") +\n    facet_grid(winner~winner12)\n\n\n\n\n\n\n\n\nIf you want to facet using only one variable, put a dot at where the other variable would stand otherwise…\n\nsocviz::county_data |&gt; \n  drop_na() |&gt; \n  ggplot() +\n    geom_point(aes(x = per_dem_2016, y = per_gop_2016, color = white)) +\n    scale_x_continuous(limits = c(0, 1)) +\n    scale_y_continuous(limits = c(0, 1)) +\n    scale_color_gradient(low = \"green\",\n                         high = \"red\") +\n    facet_grid(.~winner)\n\n\n\n\n\n\n\n\n… or just use facet_wrap().\n\n\n\nExporting graphics\nIf you include the graphics in an RMarkdown document, make sure you use the proper chunk options (i.e., {r echo=FALSE, message=FALSE, warning=FALSE}).\nIf you, however, want to export it and put it into an MS Word document or so, you can just use the ggsave() function. By default, it just takes the last plot that has been created and saves it to a path that needs to be specified. If it contains a file extension, ggsave() just uses this one.\n\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point()\n\nggsave(\"mtcars.pdf\", device = \"pdf\") #save it to pdf\nggsave(\"mtcars.png\") #save it to png\n\nggsave(\"mtcars.pdf\", width = 4, height = 4) #specify width and height -- in inches by default\nggsave(\"mtcars.pdf\", width = 20, height = 20, units = \"cm\") #change unit using the units argument\n\n\n\nFurther readings\n\nggplot2 cheatsheet.\nggplot2 – the book.\nThe graphic cookbook for R.\nAnother tutorial.\nA full-on online course by Kieran Healy (comes with an R package as well).\nNeed some inspiration? Check out the graph gallery.\nThe ggsave() function in further detail.\nYou can also consult the introverse package. introverse::show_topics(\"ggplot2\") will give you overviews of the respective package’s functions, and get_help(\"name of function\") will help you with the respective function.\n\n\n\nExercises\nTake the IMDb file.\nTry to think about how you could answer the following questions graphically. If you fail, take a look at the hints.\n\nDo higher rated movies generate more revenue?\n\nPlot revenue and rating as a scatter plot.\nDo you think there is a correlation? How could you make stronger claims about it? Maybe even graphically?\nInterpret the plot.\nAdd a nice title and labels.\n\nHow evenly are the different years’ movies represented? (Why would it be pointless to make claims about the productivity of directors?)\n\nMake a bar plot.\nInterpret the plot.\nAdd a nice title and labels.\n\nWhich year was the best for cinema fetishists? (When could they watch the most highest rated movies?)\n\nMake a box plot.\nInterpret the plot.\nAdd a nice title and labels."
  },
  {
    "objectID": "2_r_catch-up.html#iteration",
    "href": "2_r_catch-up.html#iteration",
    "title": "Chapter 2: Brief R Recap",
    "section": "Iteration",
    "text": "Iteration\nWe also will work with lists. Lists can contain elements of different lengths (which distinguishes them from tibbles). This makes them especially suitable for web scraping. Other than (atomic) vectors they are not just vectorized since they can contain elements of all different kinds of format.\nTo iterate over lists, we have the map() family from the purrr package, which applies functions over lists. pluck() extracts elements from the list.\n\nraw_list &lt;- list(first_element = 1:4, 4:6, 10:42)\nstr(raw_list) # shows you the elements of the list\n\nList of 3\n $ first_element: int [1:4] 1 2 3 4\n $              : int [1:3] 4 5 6\n $              : int [1:33] 10 11 12 13 14 15 16 17 18 19 ...\n\nmap(raw_list, mean)\n\n$first_element\n[1] 2.5\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 26\n\nmap(raw_list, ~{mean(.x) |&gt; sqrt()})\n\n$first_element\n[1] 1.581139\n\n[[2]]\n[1] 2.236068\n\n[[3]]\n[1] 5.09902\n\nmap_dbl(raw_list, mean) # by specifying the type of output, you can reduce the list\n\nfirst_element                             \n          2.5           5.0          26.0 \n\nraw_list |&gt; pluck(1) == raw_list |&gt; pluck(\"first_element\")\n\n[1] TRUE TRUE TRUE TRUE\n\n\nThis can also be achieved using a loop. Here, you use an index to loop over objects and do something to their elements. Typically, you create an empty list before and put the new output at the respective new position.\n\nnew_list &lt;- vector(mode = \"list\", length = length(raw_list))\nfor (i in seq_along(raw_list)){\n  new_list[[i]] &lt;- mean(raw_list[[i]])\n}"
  },
  {
    "objectID": "2_r_catch-up.html#functionalprogramming",
    "href": "2_r_catch-up.html#functionalprogramming",
    "title": "Chapter 2: Brief R Recap",
    "section": "Flow Control, Functional programming, and iterations",
    "text": "Flow Control, Functional programming, and iterations\nSo far, you have learned heaps of data wrangling and analyses, but no real customization of R. This will change now, as you will be introduced to functions. Furthermore, the operations have only been applied to one singular object (read vector or data.frame/tibble). Iteration means that you perform the same operation on multiple objects/data sets/you name it.\nToday’s session will all be about following the DRY principle. DRY stands for Don’t Repeat Yourself.\n“Why not?,” you may ask. Well, the problem with copy-and-pasting code is that you have to change all the variable names in every instance of your code. RStudio has a nice Search-and-Replace function which might facilitate that, but this practice still bears the danger of writing code that contains errors. This is where you will need to make use of the tools that R offers to iterate over a couple of elements, perform operations on them, and return the results. An example:\n\nexample_strings &lt;- c(\"this\", \"is\", \"how\", \"a\", \"for\", \"loop\", \"works\")\n\nfor (i in seq_along(example_strings)) {\n  print(example_strings[[i]])\n}\n\n[1] \"this\"\n[1] \"is\"\n[1] \"how\"\n[1] \"a\"\n[1] \"for\"\n[1] \"loop\"\n[1] \"works\"\n\n\nAnother option – from the tidyverse – is the purrr package:\n\nwalk(example_strings, print)\n\n[1] \"this\"\n[1] \"is\"\n[1] \"how\"\n[1] \"a\"\n[1] \"for\"\n[1] \"loop\"\n[1] \"works\"\n\n\nSo, what has this code done? In both cases, it has taken the function print() and applied it to every element of our vector. Copying-and-pasting would have looked like this:\n\nprint(example_strings[[1]])\n\n[1] \"this\"\n\nprint(example_strings[[2]])\n\n[1] \"is\"\n\nprint(example_strings[[3]])\n\n[1] \"how\"\n\nprint(example_strings[[4]])\n\n[1] \"a\"\n\nprint(example_strings[[5]])\n\n[1] \"for\"\n\nprint(example_strings[[6]])\n\n[1] \"loop\"\n\nprint(example_strings[[7]])\n\n[1] \"works\"\n\nprint(example_strings[[7]])\n\n[1] \"works\"\n\n\nDamn, I pasted the last instance twice. In this case, the mistake is obvious, but oftentimes it is not.\nIn the following, I will provide you a more extensive introduction into conditional statements, functions, loops, and the purrr (and it’s parallelized counter-part furrr) package.\n\nFlow control\nSometimes you want your code to only run in specific cases. For mutate(), I have already showed you conditional imputation of values with case_when(). A more generalized approach for conditionally running code in R are if statements. They look as follows:\n\nif (conditional_statement evaluates to TRUE) {\n  do_something\n}\n\nThey also have an extension – if…else:\n\nif (conditional_statement evaluates to TRUE) {\n  do_something\n} else {\n  do_something_else\n}\n\nImagine that I want R to tell me whether a number it draws is smaller than or equal to five:\n\nset.seed(1234)\nx &lt;- sample(10, 1)\n\nif (x &lt;= 5) {\n  print(\"x is smaller than or equals 5\")\n} \n\nIn this case, x is 3, so the if statement returns something. If this is not the case, nothing happens:\n\nset.seed(12345)\nx &lt;- sample(10, 1)\n\nif (x &lt;= 5) {\n  print(\"x is smaller than or equals 5\")\n}\n\n[1] \"x is smaller than or equals 5\"\n\n\nNow I could extend it by another if statement:\n\nset.seed(1234)\n\nx &lt;- sample(10, 1)\n\nif (x &lt;= 5) {\n  print(\"x is smaller than or equals 5\")\n}\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n\nHere, x is 10, so only the second if statement returns something.\nBut the else allows me to take a shortcut and write it more concisely:\n\nif (x &lt;= 5) {\n  print(\"x is smaller than or equals 5\")\n} else {\n  print(\"x is greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n\nPlease note that the condition inside the if statement needs to be a vector of type logical (hence, either TRUE or FALSE). Apart from that, only vectors of length 1 are allowed. The following will not work:\n\nif (c(TRUE, FALSE, TRUE)) {\n  print(\"example\")\n} #This will throw an error!!!\n\n\n\nFunctions\nSo far, every call you have made within R contained a function. Even the most basic operations, such as c() for building vectors, rely on functions. Functions are the verbs of R, they do something to your objects. Hence, you as someone who obeys the principles of DRY can make good use of them. Whenever you need to copy code to perform certain tasks to an object, you can also put those tasks into a function and just provide the function with the objects.\nImagine you want to rescale some variables in a tibble (an example I took from the OG version of R4DS (wickham2016a?)):\n\nset.seed(1234)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$b, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\nGiven that you now know how to loop over the tibble, you can certainly reduce the amount of copy-pasting here.\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nfor (i in seq_along(df)) {\n  df[[i]] &lt;- (df[[i]] - min(df[[i]], na.rm = TRUE)) / \n  (max(df[[i]], na.rm = TRUE) - min(df[[i]], na.rm = TRUE))\n}\n\nHowever, the operation within the loop is generalizable: it always only takes a vector of numeric values as input, performs some actions on them and returns another vector of the same length, but rescaled into a range from 0 to 1. Hence, the operation fulfills the requirements for putting it into a function.\nDoing so has some advantages:\n\nIf an error occurs, you can simply change the function in one place – when you define it – instead of changing all the occurrences in your code\nIt will certainly make your code easier to read – rescale0to1 is a more concise description than (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) (–&gt; you see what I did here? I already replaced the arguments with a generic variable. You can use it to write the function yourself.)\n\n\nWriting your own functions\nWhen you define functions in R, you need to follow a certain structure:\n\nfunction_name &lt;- function(argument_1, argument_2, argument_n) {\n  function_body\n}\n\n\nThe function_name is the thing you will call (e.g., mean()). In general, it should be a verb, it should be concise, and it should be in_snakecase.\nThe arguments are what you need to provide the function with (e.g., mean(1:10)).\nThe function body contains the operations which are performed to the arguments. It can contain other functions as well – which need to be defined beforehand (e.g., sum(1:10) / length(1:10))). It is advisable to split up the function body into as little pieces as you can.\n\n\n\nAn example: Roulette\nIn the following, I will guide you through a quick example on how you could use functions to play an extremely basic game of Roulette with R. You provide it with two values (how much you bet and which number you choose) and R takes care of the rest.\nSo what does the function need to do? First, it needs to draw a number between 0 and 36. Second, it needs to compare the bet and its corresponding number. Third, it needs to return the respective result.\n\nplay_roulette &lt;- function(bet, number) {\n  draw &lt;- sample(0:36, 1)\n  tibble(\n    winning_number = draw,\n    your_number = number,\n    your_bet = bet,\n    your_return = if (number == draw) {\n      bet * 36\n    } else {\n      0\n    }\n  )\n}\n\nplay_roulette(bet = 1, number = 35)\n\n# A tibble: 1 × 4\n  winning_number your_number your_bet your_return\n           &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1             15          35        1           0\n\n\nBut how to make sure that I do not bet on a number which I cannot bet on (i.e., numbers greater than 36)? Or, put differently, how to forbid values? Use stop(). Besides, how to set default values for the arguments? Just use argument = default.\n\nplay_roulette_restricted &lt;- function(bet = 1, number) {\n  if (number &gt; 36) stop(\"You can only bet on numbers between 0 and 36.\")\n  draw &lt;- sample(0:36, 1)\n  tibble(\n    winning_number = draw,\n    your_number = number,\n    your_bet = bet,\n    your_return = if (number == draw) {\n      bet * 36\n    } else {\n      0\n    }\n  )\n  #return(tbl_return)\n}\nplay_roulette_restricted(number = 3)\n\n# A tibble: 1 × 4\n  winning_number your_number your_bet your_return\n           &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1              1           3        1           0\n\n\nThe function returns the results of the last call, i.e., the tibble. If you want to be more concrete about what it should return – or make an earlier return – use return(). The function will stop as soon as it hits a return() statement.\n\nplay_roulette_basic &lt;- function(bet = 1, number) {\n  if (number &gt; 36) stop(\"You can only bet on numbers between 0 and 36.\")\n  draw &lt;- sample(0:36, 1)\n  if (number == draw) {\n    return(str_c(\"Nice, you won\", as.character(bet * 36), \"Dollars\", sep = \" \"))\n  } else {\n    return(\"I'm sorry, you lost.\")\n  }\n}\nplay_roulette_basic(number = 35)\n\n[1] \"I'm sorry, you lost.\"\n\n\n\n\nFunctional programming with tidyverse functions\nThe majority of dplyr verbs uses so-called tidy evaluation which is a framework for controlling how expressions and variables in your code are evaluated by the tidyverse functions. The two main things here are data masking and tidy selection. The former facilitates computing on values within the data set and refers to functions such as filter(), where you can just type in variable names instead of tediously typing name_of_df$var_name. The latter aims to facilitate working with the columns in the data set. It is provided by the tidyselect package and allows you, for instance, to work with code such as tbl |&gt; select(starts_with(\"a\")). More examples can be acquired using ?dplyr_tidy_select.\nI will not go into detail here but rather stick to what implications this has to you. If you are interested in the theoretical underpinnings, read the chapter on “Metaprogramming” in Advanced R by Hadley Wickham.\nIf your function takes a user-supplied variable as an argument, you need to consider this arguments in the pipeline. For instance, the following function calculates the mean, median, and standard deviation of a variable.\n\nmy_summary &lt;- function(tbl, var) {\n  tbl |&gt; \n    summarize(\n      mean = mean({{ var }}),\n      median = median({{ var }}),\n      sd = sd({{ var }})\n    )\n}\n\nmtcars |&gt; my_summary(cyl) \n\n    mean median       sd\n1 6.1875      6 1.785922\n\n\nIf the variable names are supplied in a character vector, you need all_of():\n\nsummarize_mean &lt;- function(data, vars) {\n  data |&gt; summarize(n = n(), across({{ vars }}, mean))\n}\n\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarize_mean(all_of(c(\"hp\", \"mpg\"))) |&gt; \n  glimpse()\n\nRows: 3\nColumns: 4\n$ cyl &lt;dbl&gt; 4, 6, 8\n$ n   &lt;int&gt; 11, 7, 14\n$ hp  &lt;dbl&gt; 82.63636, 122.28571, 209.21429\n$ mpg &lt;dbl&gt; 26.66364, 19.74286, 15.10000\n\n\nAnother handy thing is changing the variable names in the output depending on the input names. Here, you can use glue syntax and :=:\n\nmy_summary_w_names &lt;- function(tbl, var){\n  tbl |&gt; \n    summarize(\n      \"mean_{{ var }}\" := mean({{ var }}),\n      \"median_{{ var }}\" := median({{ var }}),\n      \"sd_{{ var }}\" := sd({{ var }})\n    )\n}\n\nmtcars |&gt; my_summary_w_names(cyl)\n\n  mean_cyl median_cyl   sd_cyl\n1   6.1875          6 1.785922\n\n\nFind more on programming with dplyr in this vignette.\n\n\nFurther readings\nIf you want to learn more about functional programming, check out the following resources:\n\nThe R4DS chapter\nA basic tutorial\nA book chapter about control-flow and functions\nHadley on functional programming\n\n\n\n\nIteration\nStrictly speaking, there are three kinds of loops: for, repeat, and while. I will touch upon for and while, because they are more straight-forward than repeat. repeat loops will repeat a task until you tell it to stop by hitting the escape button or adding a condition up front. Interactive programming – sitting in front of your machine and hitting the escape button to break a loop – is no desired practice and while loops have internalized the condition already. Hence, repeat loops do not appear to have any advantage and I leave them out deliberately.\n\nfor loops\nfor loops are the sort of loops you will have to work with more often as they allow you to loop over a predefined number of elements. For this sake, I will briefly revise how you index vectors, lists, and tibbles.\nThe ith element of a vector can be accessed by using either [[i]] or [i].\nThe ith element of a list can be obtained by using [[i]] – [i] would return a sub-list instead of the element. The second element of the ith element in a list (if it were a vector or a list) can be obtained using [[i]][[2]] etc.\nThe ith column of a tibble can be accessed as a vector using [[i]]. The second value of the ith column of a tibble can be accessed using [[i]][[2]]\nHow does that matter for for loops? Remember the example I showed you in the beginning? All a for loop does is iterating over a vector of values and imputing them instead of a placeholder.\n\nexample_strings &lt;- c(\"this\", \"is\", \"how\", \"a\", \"for\", \"loop\", \"works\")\n\nfor (i in example_strings) {\n  print(i)\n}\n\n[1] \"this\"\n[1] \"is\"\n[1] \"how\"\n[1] \"a\"\n[1] \"for\"\n[1] \"loop\"\n[1] \"works\"\n\n\nFor a more general approach, you can also loop over the indices of the vector using seq_along() which creates a sequence along a vector.\n\nseq_along(example_strings) # seq_along looks like this\n\n[1] 1 2 3 4 5 6 7\n\n\nThis is especially useful if you want to modify the elements of a vector or a tibble.\n\nfor (i in seq_along(example_strings)) {\n  print(example_strings[[i]])\n}\n\n[1] \"this\"\n[1] \"is\"\n[1] \"how\"\n[1] \"a\"\n[1] \"for\"\n[1] \"loop\"\n[1] \"works\"\n\n#Hence, the first iteration looks like this.\nprint(example_strings[[seq_along(example_strings)[[1]]]])\n\n[1] \"this\"\n\n# translates to\nprint(example_strings[[1]])\n\n[1] \"this\"\n\n\nNow that you have seen the general approach for using a for loop, how can you use them in practice for data manipulation? Whenever you use a for loop, you need to follow a three step approach:\n\nOutput: In the beginning, you need to create a vector that we can fill with output. You also need to determine the length of the vector in the beginning. This is due to efficiency: if you were to grow the vector by every iteration (using c), the loop becomes very slow. This is especially important if you work with large data sets. An example for creating an empty vector of a certain length is output &lt;- vector(mode = \"numeric\", length = length_of_output).\nSequence: i in seq_along(variable) tells the for loop what to loop over.\n\nBody: The actual code. Performs the operation on the respective instance and stores the resulting value in the pre-defined output vector at position i.\n\nfor loops are considered slow. They are not, at least not if you stick to the following rules:\n\nAlways pre-allocate space – make sure that R does not have to expand your objects\nDo as much as you can outside the loop – every operation inside the loop will be repeated every time the loop is repeated\n\nIn general, you will come across three different problems with for loops.\n\nModifying an existing object\nLength of output is unknown\nSequences are of unknown length\n\n\nModifying the existing object\nSometimes you want to modify an existing object rather than creating a new one. This is useful when working with large datasets or when you need to update values in place.\nBasic example: Standardizing columns Let’s say you have a dataset with test scores that you want to standardize (mean = 0, sd = 1):\n\nscores &lt;- tibble(\n  student_id = 1:5,\n  math = c(85, 92, 78, 88, 95),\n  science = c(90, 85, 80, 92, 88),\n  english = c(88, 90, 85, 86, 92)\n)\n\nfor (col in c(\"math\", \"science\", \"english\")) {\n  scores[[col]] &lt;- (scores[[col]] - mean(scores[[col]])) / sd(scores[[col]])\n}\n\nscores |&gt; \n  summarize(across(c(math, science, english), \n                   list(mean = mean, sd = sd)))\n\n# A tibble: 1 × 6\n  math_mean math_sd science_mean science_sd english_mean english_sd\n      &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1  8.88e-16       1     7.22e-17          1    -9.77e-16          1\n\n\n\n\nLength of output is unknown\nSometimes, you do not know how long your output object is. This is, for instance, if you simulate vectors of random length. Normally, you would just put the values into a vector. However, if you do not know the length, then you would have to ask R to grow the vector every iteration. But this is extremely inefficient.\nFor this, the solution is lists. You always know how many iterations your loop will have. Hence, you can create a list of this exact length and then just store the results in the list (as lists do not care about the length of the singular elements). Afterwards, you can unlist() or flatten_*() the list into a vector.\n\na_list &lt;- vector(mode = \"list\", length = 10L)\n\nfor (i in seq_along(a_list)) {\n  len &lt;- sample(1:10, 1)\n  a_list[[i]] &lt;- rnorm(len)\n}\n\na_list |&gt; \n  unlist() # or unlist(a_list)\n\n [1]  0.69760871  0.54999735 -0.40273198 -0.19159377 -1.19452788 -0.05315882\n [7] -1.43192024 -2.01016571  0.33832922  0.65128696  2.43315152  1.19133821\n[13]  0.92244282  0.62109737  0.35633348 -0.47471847  0.06599349 -0.50247778\n[19]  2.18711916 -0.58172745  0.70008023  1.49217658 -0.05210512 -0.19593462\n[25] -0.64906975 -1.10976723  0.84927420 -0.01394090  0.16863694  0.86926335\n[31] -0.79866986 -0.50375053  2.31559832 -0.69220912  0.49335047 -0.05760147\n[37]  1.82420830  0.08005964 -0.63140930 -1.51328812 -0.63609983  0.22630153\n[43]  1.01369035 -1.17194831  0.66871433 -1.65010093 -0.36585225 -0.31611833\n[49] -1.94824605  0.92005752 -0.62287159 -0.17827861  0.78695133 -0.58054783\n[55]  0.91825114\n\n\nIf we wanted to add the information in a tibble, we could add it during the run and use bind_rows() afterwards.\n\na_list &lt;- vector(mode = \"list\", length = 10L)\n\nfor (i in seq_along(a_list)) {\n  len &lt;- sample(1:10, 1)\n  a_list[[i]] &lt;- list(\n    run = i,\n    values = rnorm(len)\n  )\n}\n\ndf &lt;- a_list |&gt; \n  bind_rows()\n\ndf\n\n# A tibble: 55 × 2\n     run  values\n   &lt;int&gt;   &lt;dbl&gt;\n 1     1  0.700 \n 2     1 -1.20  \n 3     1 -0.499 \n 4     2  0.0976\n 5     3 -0.119 \n 6     3  2.39  \n 7     3  0.735 \n 8     3  0.474 \n 9     3 -0.234 \n10     3 -0.854 \n# ℹ 45 more rows\n\n\n\n\nUnknown sequence length\nSometimes, you also do not know how long your input sequence is. Instead, you want to loop until a certain condition is met. This is for instance the case when looping across multiple pages in web-scraping. Here, while loops come in handy (but this is the only use case I could think of).\nThe basic structure of while loops is as follows:\n\nwhile (condition) {\n  code\n}\n\nWhat could an example look like?4 The following loop keeps running until three heads appeared in a row and the condition is met.\nPlease note that both vectors which are to be modified within the loop – indicator and head – need to be created beforehand. If I had not created head beforehand, the loop would not have started because there would not have been any vector to assess the length.\n\nindicator &lt;- 0\nhead &lt;- c()\nwhile (length(head) &lt; 3) {\n  if (sample(2, 1) == 1) {\n    x &lt;- \"head\"\n  } else {\n    x &lt;- \"tail\"\n  }\n  if (x == \"head\") {\n    head &lt;- c(head, 1)\n  } else {\n    length(head) &lt;- 0\n  }\n  indicator &lt;- indicator + 1\n}\n\nIn this case, you still want to pre-allocate space. Hence, you could also use a list here. You can just do a very long list and afterwards cut it down to size using purrr::compact().\n\nindicator &lt;- 0\nvalues &lt;- vector(mode = \"list\", length = 1000)\nhead &lt;- c()\nwhile (length(head) &lt; 3) {\n  if (sample(2, 1) == 1) {\n    x &lt;- \"head\"\n  } else {\n    x &lt;- \"tail\"\n  }\n  if (x == \"head\") {\n    head &lt;- c(head, 1)\n  } else {\n    length(head) &lt;- 0\n  }\n  values[[indicator + 1]] &lt;- x\n  indicator &lt;- indicator + 1\n}\n\nlength(values)\n\n[1] 1000\n\nvalues |&gt; tail(5) #last 5 values\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\nvalues |&gt; compact() #removes all NULL elements\n\n[[1]]\n[1] \"tail\"\n\n[[2]]\n[1] \"head\"\n\n[[3]]\n[1] \"head\"\n\n[[4]]\n[1] \"tail\"\n\n[[5]]\n[1] \"head\"\n\n[[6]]\n[1] \"head\"\n\n[[7]]\n[1] \"head\"\n\n\n\n\n\npurrr::map()\nLoops are good because they make everything very explicit. However, it is often tedious to type. The purrr package provides functions which enable you to iterate over vectors, data frames/tibbles, and lists. Apart from that, it has a lot of functions to work with lists as well. I will only cover the former functions. If you are interested in using purrr for working with lists, check out this extensive tutorial by Jenny Bryan.\nIn the beginning of this chapter, I used the walk() function. This function is related to map() as it iterates over a vector and applies a function to its respective elements. The difference is that walk() doesn’t store the results, map() does.\n\nThe basics\nThe structure of the map() function looks like this:\n\nmap(vector or list, function(, if you need it, additional arguments of function))\n\nmap() always returns a list.\nIf you want the output to be in a different format, there are different, type-specific map() functions.\n\nmap_dfr() returns a data frame – by binding the rows\nmap_dfc() returns a data frame – by binding the columns\nmap_dbl() returns a double vector\nmap_chr() returns a character vector\nmap_lgl() returns a logical vector\n\nIn the following I will demonstrate the function of map() with a simple example. The basic vector I will map over is:\n\nexample_dbl &lt;- c(1.5, 1.3, 1.8, 1.9, 2.3)\n\nIn the first example, I just add 10 to the vector. In order to do so, I first need to create a function which adds 10.\n\nadd_10 &lt;- function(x) {\n  x + 10\n}\n\n\nmap(example_dbl, add_10)\n\n[[1]]\n[1] 11.5\n\n[[2]]\n[1] 11.3\n\n[[3]]\n[1] 11.8\n\n[[4]]\n[1] 11.9\n\n[[5]]\n[1] 12.3\n\n\n\nmap_dbl(example_dbl, add_10)\n\n[1] 11.5 11.3 11.8 11.9 12.3\n\n\n\nmap_chr(example_dbl, add_10) # does not make sense though\n\nWarning: Automatic coercion from double to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\n\n[1] \"11.500000\" \"11.300000\" \"11.800000\" \"11.900000\" \"12.300000\"\n\n\n\n\nAnonymous functions\nIn the former example, I did specify the function beforehand. map() also allows you to define the function within the call using a so-called anonymous function \\(x). The function’s argument is pre-defined (x in this case, but could be any placeholder) which stands for the respective input.\n\nmap_dbl(example_dbl, \\(x){\n  x + 10\n  })\n\n[1] 11.5 11.3 11.8 11.9 12.3\n\n\nYou can also map across tibbles. Here, you iterate over columns. For instance, calculating a mean for each column of the cars_tbl would have looked like this in purrr:\n\ncars_tbl &lt;- mtcars\nmap(cars_tbl, mean)\n\n$mpg\n[1] 20.09062\n\n$cyl\n[1] 6.1875\n\n$disp\n[1] 230.7219\n\n$hp\n[1] 146.6875\n\n$drat\n[1] 3.596563\n\n$wt\n[1] 3.21725\n\n$qsec\n[1] 17.84875\n\n$vs\n[1] 0.4375\n\n$am\n[1] 0.40625\n\n$gear\n[1] 3.6875\n\n$carb\n[1] 2.8125\n\n\nWhen I put it into a tibble, names are preserved:\n\nmap_dfc(cars_tbl, mean)\n\n# A tibble: 1 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  20.1  6.19  231.  147.  3.60  3.22  17.8 0.438 0.406  3.69  2.81\n\n\n\n\nMapping over multiple arguments\nSometimes you want to apply things to multiple arguments. Think for example of the sample()function. It requires at least two arguments: the size of the sample you draw and the element space x you draw the sample from.\n\nmap2(10, 1:5, sample, replace = TRUE)\n\n[[1]]\n[1] 4\n\n[[2]]\n[1]  1 10\n\n[[3]]\n[1]  7  6 10\n\n[[4]]\n[1] 1 6 6 6\n\n[[5]]\n[1] 7 5 5 5 6\n\n\nHowever, the map2() functions do not provide you with the possibility to control the type of output you get. You can take care of this using flatten_*().\n\nmap2(10, 5, sample) |&gt; flatten_dbl()\n\n[1] 6 4 8 3 2\n\n\nIf you provide it with a vector which is longer than 1, map2() will not perform the operation on every possible combination of the two vectors. Instead, it iterates over both vectors simultaneously, hence, the first iteration uses the first two values, the second iteration the second two values etc. Also note that it matches the arguments by position, not by name, hence the second argument is the size of the sample, the first one the element space.\n\nmap2(c(10, 5), c(5, 3), sample) \n\n[[1]]\n[1]  7  8  4  5 10\n\n[[2]]\n[1] 1 3 5\n\n\nIf you want to use an anonymous function, you can do so as well:\n\nmap2(c(10, 5), c(5, 3), \\(x, y) sample(x, size = y))\n\n[[1]]\n[1] 4 8 6 7 2\n\n[[2]]\n[1] 5 1 3\n\n\nIf you want to map over more than two arguments, pmap() is the way to go. If you work with functions which need multiple values as arguments, you can store the vectors containing the respective values in a tibble. You should name the columns according to the function’s arguments.\nAn example here is drawing numbers from a normal distribution – rnorm(). The function takes three arguments: n– the number of values to be drawn, mean, and sd.\n\ntibble(\n  n = 10,\n  mean = 1:10,\n  sd = 0.5\n) |&gt; \n  pmap(rnorm)\n\n[[1]]\n [1] 0.2762588 1.2662280 0.4966849 1.3585467 0.7492659 0.7907689 0.3105248\n [8] 1.1213886 1.4226768 0.1782507\n\n[[2]]\n [1] 2.509489 2.476223 2.410380 2.813266 2.116919 2.392606 1.324271 1.782913\n [9] 1.659243 2.718527\n\n[[3]]\n [1] 3.433201 2.123642 2.645378 3.158825 3.418090 4.271263 3.200257 3.017893\n [9] 3.141365 2.606872\n\n[[4]]\n [1] 3.914835 4.595926 4.682905 3.619700 3.513134 4.954899 3.123541 4.477846\n [9] 5.167031 4.550836\n\n[[5]]\n [1] 5.545990 5.142692 4.220136 5.427874 5.213967 4.856378 4.899412 5.450255\n [9] 3.873000 4.740156\n\n[[6]]\n [1] 5.818541 6.971764 6.147575 6.582609 6.121836 6.226723 6.257705 5.418025\n [9] 5.531128 6.076174\n\n[[7]]\n [1] 6.859280 6.594688 6.866800 7.426123 6.291120 6.845510 7.212836 6.980094\n [9] 6.177029 7.468289\n\n[[8]]\n [1] 8.743606 8.803838 8.125447 8.136227 8.075560 8.486074 8.504599 7.792865\n [9] 7.557125 8.727168\n\n[[9]]\n [1] 8.815799 9.599489 9.429286 9.243450 8.937363 9.010900 8.082863 9.765383\n [9] 9.820104 8.174236\n\n[[10]]\n [1] 10.015095  9.715463 10.305483 10.754284  9.661376 10.500636 10.617799\n [8] 10.823392  9.387472  9.728401\n\n\nIf you want to use an anonymous function, you can do so as well:\n\ntibble(\n  n = 10,\n  mean = 1:10,\n  sd = 0.5\n) |&gt; \n  pmap(\\(n, mean, sd) rnorm(n, mean, sd))\n\n[[1]]\n [1] 0.7772323 0.0416125 0.9735931 0.9834894 1.0114624 0.3249466 1.0569058\n [8] 1.3214937 1.1249126 0.9573270\n\n[[2]]\n [1] 1.4034810 2.1781413 2.0860438 2.9868643 1.4047264 1.5553486 1.5861139\n [8] 1.3575352 1.3621371 0.7242431\n\n[[3]]\n [1] 4.093959 2.798665 4.107978 1.874788 3.051627 3.409510 3.433766 3.728983\n [9] 2.355771 2.556102\n\n[[4]]\n [1] 2.433150 4.199967 4.212392 4.328054 4.488375 4.602977 4.691167 4.146277\n [9] 4.189498 4.097553\n\n[[5]]\n [1] 5.140349 5.092283 4.738515 5.529182 4.269838 5.013457 4.620869 4.823738\n [9] 4.207710 4.723280\n\n[[6]]\n [1] 6.245906 5.058408 5.516441 5.472807 6.025419 5.685291 6.462158 6.550718\n [9] 6.566605 5.717003\n\n[[7]]\n [1] 7.087744 6.923719 6.899486 6.571321 7.224902 6.843166 7.430276 6.926348\n [9] 6.838748 6.860724\n\n[[8]]\n [1] 8.402492 7.871721 7.946986 8.426621 8.797195 8.227491 7.582142 8.157361\n [9] 8.107146 7.558428\n\n[[9]]\n [1] 9.694184 9.786892 9.286632 9.995288 8.535659 9.185741 8.114063 9.860151\n [9] 8.876233 8.891865\n\n[[10]]\n [1] 10.696122 11.047684 10.039567 10.655035  9.739268  9.559926  9.317054\n [8] 10.097944  9.665217 10.051768\n\n\n\n\nSpeeding up with furrr\nIf you work with large data sets or have to perform a lot of iterations, you might want to speed up your code. The furrr package provides the same functionality as purrr, but allows for parallelization. This means that it can split up the tasks and distribute them across multiple CPU cores.\nIts functionalities are the same as in purrr, just with a future_ prefix. In order to use it, you need to set up a plan first. Here, I use multisession, which works on all platforms (Windows, Mac, Linux). If you work on a Linux machine, you can also use multicore, which is faster.\n\nneeds(furrr)\nplan(multisession) #set up parallelization\nfuture_map_dbl(example_dbl, add_10)\n\n[1] 11.5 11.3 11.8 11.9 12.3\n\n\nNote that speed benefits are not apparent when working with small data sets or few iterations. The overhead of setting up parallel processes can outweigh the benefits. However, if you work with large data sets or have to perform a lot of iterations, you will see a speed increase.\nLet’s make add_10 slow and benchmark furrr and purrr using the tictoc package. I have set the sleep time to 0.5 seconds to make the difference more apparent. My laptop has 8 cores, hence I create 24 tasks which will be distributed across the 8 workers (3 tasks per worker).\n\nneeds(tictoc)\n\n# Create more tasks than workers\nexample_long &lt;- 1:24 \n\nadd_10_slow &lt;- function(x) {\n  Sys.sleep(0.5)  # Shorter sleep time\n  x + 10\n}\n\n# Sequential version\ntic(\"Sequential (purrr)\")\nresult_seq &lt;- map_dbl(example_long, add_10_slow)\ntoc()\n\nSequential (purrr): 12.087 sec elapsed\n\n# Parallel version\ntic(\"Parallel (furrr)\")\nresult_par &lt;- future_map_dbl(example_long, add_10_slow)\ntoc()\n\nParallel (furrr): 3.03 sec elapsed\n\n\nYou can see that the parallel version is much faster than the sequential one. However, the speed increase is not linear. This is due to the overhead of setting up parallel processes. Setting up worker processes, transferring data between them, and collecting results all take time. With my 0.5 second tasks, this overhead becomes a significant fraction of the total runtime. Also, following Amdahl’s Law, not everything can be parallelized. Some parts like initial setup and final result collection must run sequentially. This creates a fundamental limit on speedup. Finally, there are system constraints, as my CPU shares resources with other processes, memory bandwidth can become a bottleneck, and modern CPUs can’t maintain peak single-core performance across all cores simultaneously.\nSo, when should we use furrr?\n\nWhen you have a large number of tasks that can be executed independently.\nWhen each task takes a significant amount of time to complete.\nWhen you have access to a multi-core machine.\n\nWhen should we avoid furrr?\n\nWhen tasks are very quick to execute (the overhead of parallelization may outweigh the benefits).\nWhen tasks depend on each other (parallelization won’t help).\nWhen working in an environment where parallel processing is restricted (e.g., some cloud services or shared servers).\nWhen debugging (parallel code can be harder to debug).\n\n\n\n\nFurther links\n\nChapter about loops in Hands-on Programming with R\nOn control flow\nA basic introduction to purrr::map\nThe corresponding chapter in R4DS"
  },
  {
    "objectID": "2_r_catch-up.html#footnotes",
    "href": "2_r_catch-up.html#footnotes",
    "title": "Chapter 2: Brief R Recap",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis becomes especially painful if you teach R to your students and have to grade 20 submissions and, hence, have to paste your personal directory’s file path into each of these submissions.↩︎\nwhich can be found here or using vignette(\"tidy-data\", package = \"tidyr\")↩︎\nIf you want the code on your machine, download the files behind the following links and store them in a folder called socviz_us which is again stored in a folder named data which lives in the same folder as the .qmd file. https://www.dropbox.com/s/14k6bkmaq6l47p2/midwest.csv?dl=0 ; https://www.dropbox.com/s/t3057jf9evt6vjz/northeast.csv?dl=0 ; https://www.dropbox.com/s/lbdde4udlrfea46/south.csv?dl=0 ; https://www.dropbox.com/s/vcvl90dbegagv4z/west.csv?dl=0↩︎\nI have taken this example from the R for Data Science book. I hardly ever work with while loops. The only use case from my day-to-day work is web-scraping, where I want to loop over pages until a certain threshold is reached. Therefore, I could not really come up with an example myself.↩︎"
  }
]